{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by: \n",
    "\n",
    "Date: 2024-12-07 \n",
    "\n",
    "Latest change when and what:\n",
    "\n",
    "Notes:\n",
    "\n",
    "# 2. Preprocessing\n",
    "\n",
    "Cleaning the data, tokenizing it, splitting it into test, train and validation, and finally embedding the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import kagglehub\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nosferatu, eine Symphonie des Grauens</td>\n",
       "      <td>2</td>\n",
       "      <td>13442</td>\n",
       "      <td>1922</td>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "      <td>Nosferatu_0013442.txt</td>\n",
       "      <td>\\n\\n                              1922\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phantom of the Opera, The</td>\n",
       "      <td>2</td>\n",
       "      <td>16220</td>\n",
       "      <td>1925</td>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "      <td>The Phantom of the Opera_0016220.txt</td>\n",
       "      <td>The Phantom of the Opera\\n\\nTHE PHANTOM OF THE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Battleship Potemkin</td>\n",
       "      <td>0</td>\n",
       "      <td>15648</td>\n",
       "      <td>1925</td>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "      <td>Battleship Potemkin_0015648.txt</td>\n",
       "      <td>Battleship Potemkin\\n\\nScenario and script by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lost World, The</td>\n",
       "      <td>2</td>\n",
       "      <td>16039</td>\n",
       "      <td>1925</td>\n",
       "      <td>5514</td>\n",
       "      <td>0</td>\n",
       "      <td>The Lost World_0016039.txt</td>\n",
       "      <td>THE LOST WORLD\\nJURASSIC PARK\\n\\nscreenplay by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metropolis</td>\n",
       "      <td>1</td>\n",
       "      <td>17136</td>\n",
       "      <td>1927</td>\n",
       "      <td>1267</td>\n",
       "      <td>0</td>\n",
       "      <td>Metropolis_0017136.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  rating  imdbid  year    id  \\\n",
       "0  Nosferatu, eine Symphonie des Grauens       2   13442  1922  1307   \n",
       "1              Phantom of the Opera, The       2   16220  1925  1305   \n",
       "2                    Battleship Potemkin       0   15648  1925  1308   \n",
       "3                        Lost World, The       2   16039  1925  5514   \n",
       "4                             Metropolis       1   17136  1927  1267   \n",
       "\n",
       "   passed_bechdel                       script_filename  \\\n",
       "0               0                 Nosferatu_0013442.txt   \n",
       "1               0  The Phantom of the Opera_0016220.txt   \n",
       "2               0       Battleship Potemkin_0015648.txt   \n",
       "3               0            The Lost World_0016039.txt   \n",
       "4               0                Metropolis_0017136.txt   \n",
       "\n",
       "                                              script  \n",
       "0  \\n\\n                              1922\\n\\n\\n\\n...  \n",
       "1  The Phantom of the Opera\\n\\nTHE PHANTOM OF THE...  \n",
       "2  Battleship Potemkin\\n\\nScenario and script by ...  \n",
       "3  THE LOST WORLD\\nJURASSIC PARK\\n\\nscreenplay by...  \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"../1_data_acquisition/data/labels_and_scripts.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning the data\n",
    "\n",
    "Removing '/n', lowercasing, removing special characters, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nosferatu, eine Symphonie des Grauens</td>\n",
       "      <td>2</td>\n",
       "      <td>13442</td>\n",
       "      <td>1922</td>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "      <td>Nosferatu_0013442.txt</td>\n",
       "      <td>1922 nosferatu cast count dracula the vampirem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phantom of the Opera, The</td>\n",
       "      <td>2</td>\n",
       "      <td>16220</td>\n",
       "      <td>1925</td>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "      <td>The Phantom of the Opera_0016220.txt</td>\n",
       "      <td>the phantom of the opera the phantom of the op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Battleship Potemkin</td>\n",
       "      <td>0</td>\n",
       "      <td>15648</td>\n",
       "      <td>1925</td>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "      <td>Battleship Potemkin_0015648.txt</td>\n",
       "      <td>battleship potemkin scenario and script by ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lost World, The</td>\n",
       "      <td>2</td>\n",
       "      <td>16039</td>\n",
       "      <td>1925</td>\n",
       "      <td>5514</td>\n",
       "      <td>0</td>\n",
       "      <td>The Lost World_0016039.txt</td>\n",
       "      <td>the lost world jurassic park screenplay by dav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metropolis</td>\n",
       "      <td>1</td>\n",
       "      <td>17136</td>\n",
       "      <td>1927</td>\n",
       "      <td>1267</td>\n",
       "      <td>0</td>\n",
       "      <td>Metropolis_0017136.txt</td>\n",
       "      <td>metropolis by corey mandell fade in ext manhat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  rating  imdbid  year    id  \\\n",
       "0  Nosferatu, eine Symphonie des Grauens       2   13442  1922  1307   \n",
       "1              Phantom of the Opera, The       2   16220  1925  1305   \n",
       "2                    Battleship Potemkin       0   15648  1925  1308   \n",
       "3                        Lost World, The       2   16039  1925  5514   \n",
       "4                             Metropolis       1   17136  1927  1267   \n",
       "\n",
       "   passed_bechdel                       script_filename  \\\n",
       "0               0                 Nosferatu_0013442.txt   \n",
       "1               0  The Phantom of the Opera_0016220.txt   \n",
       "2               0       Battleship Potemkin_0015648.txt   \n",
       "3               0            The Lost World_0016039.txt   \n",
       "4               0                Metropolis_0017136.txt   \n",
       "\n",
       "                                              script  \n",
       "0  1922 nosferatu cast count dracula the vampirem...  \n",
       "1  the phantom of the opera the phantom of the op...  \n",
       "2  battleship potemkin scenario and script by ser...  \n",
       "3  the lost world jurassic park screenplay by dav...  \n",
       "4  metropolis by corey mandell fade in ext manhat...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"script\"] = (\n",
    "    data[\"script\"]\n",
    "    .str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n",
    "    .str.replace('\\n', ' ')                   # Remove newlines\n",
    "    .str.lower()                             # Convert to lowercase - MAYBE DON'T?????\n",
    "    .str.replace(r'\\s+', ' ', regex=True)    # Replace multiple spaces with a single space\n",
    "    .str.strip()                             # Remove leading/trailing spaces\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Splitting the data into train, test and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1424\n",
      "Validation size: 178\n",
      "Test size: 179\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the data into train and temp (validation+test) sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)  # 20% for validation+test\n",
    "\n",
    "# Step 2: Split temp_data into validation and test sets (10% each)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)  # 50% of temp (10% of original)\n",
    "\n",
    "# Display the sizes of each set\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the test, train and val datasets\n",
    "train_data.to_csv(\"train.csv\", index= False)\n",
    "test_data.to_csv(\"test.csv\", index= False)\n",
    "val_data.to_csv(\"validation.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloading the datasets\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "val_data = pd.read_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing (500 tokens)\n",
    "\n",
    "Tokenizing using a pre-trained BERT tokenizer from transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_length=500: Specifies the maximum number of tokens to include.\n",
    "\n",
    "\n",
    "truncation=True: Ensures that if the text exceeds 500 tokens, it will be truncated to fit the specified length.\n",
    "\n",
    "\n",
    "add_special_tokens=True: Includes any special tokens required by the model, such as [CLS] and [SEP] for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fd8d8e62be4cc4b1cbe1e6f62ac945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08dcb73fdff84924b8f99afd1cdb9892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b8278300a6435e854c990e5b69f268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca6bc3036d844ca82712da059e4f242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>script_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nosferatu, eine Symphonie des Grauens</td>\n",
       "      <td>2</td>\n",
       "      <td>13442</td>\n",
       "      <td>1922</td>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "      <td>Nosferatu_0013442.txt</td>\n",
       "      <td>1922 nosferatu cast count dracula the vampirem...</td>\n",
       "      <td>[[CLS], 1922, nos, ##fera, ##tu, cast, count, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phantom of the Opera, The</td>\n",
       "      <td>2</td>\n",
       "      <td>16220</td>\n",
       "      <td>1925</td>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "      <td>The Phantom of the Opera_0016220.txt</td>\n",
       "      <td>the phantom of the opera the phantom of the op...</td>\n",
       "      <td>[[CLS], the, phantom, of, the, opera, the, pha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Battleship Potemkin</td>\n",
       "      <td>0</td>\n",
       "      <td>15648</td>\n",
       "      <td>1925</td>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "      <td>Battleship Potemkin_0015648.txt</td>\n",
       "      <td>battleship potemkin scenario and script by ser...</td>\n",
       "      <td>[[CLS], battleship, pot, ##em, ##kin, scenario...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lost World, The</td>\n",
       "      <td>2</td>\n",
       "      <td>16039</td>\n",
       "      <td>1925</td>\n",
       "      <td>5514</td>\n",
       "      <td>0</td>\n",
       "      <td>The Lost World_0016039.txt</td>\n",
       "      <td>the lost world jurassic park screenplay by dav...</td>\n",
       "      <td>[[CLS], the, lost, world, jurassic, park, scre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metropolis</td>\n",
       "      <td>1</td>\n",
       "      <td>17136</td>\n",
       "      <td>1927</td>\n",
       "      <td>1267</td>\n",
       "      <td>0</td>\n",
       "      <td>Metropolis_0017136.txt</td>\n",
       "      <td>metropolis by corey mandell fade in ext manhat...</td>\n",
       "      <td>[[CLS], metropolis, by, corey, man, ##dell, fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  rating  imdbid  year    id  \\\n",
       "0  Nosferatu, eine Symphonie des Grauens       2   13442  1922  1307   \n",
       "1              Phantom of the Opera, The       2   16220  1925  1305   \n",
       "2                    Battleship Potemkin       0   15648  1925  1308   \n",
       "3                        Lost World, The       2   16039  1925  5514   \n",
       "4                             Metropolis       1   17136  1927  1267   \n",
       "\n",
       "   passed_bechdel                       script_filename  \\\n",
       "0               0                 Nosferatu_0013442.txt   \n",
       "1               0  The Phantom of the Opera_0016220.txt   \n",
       "2               0       Battleship Potemkin_0015648.txt   \n",
       "3               0            The Lost World_0016039.txt   \n",
       "4               0                Metropolis_0017136.txt   \n",
       "\n",
       "                                              script  \\\n",
       "0  1922 nosferatu cast count dracula the vampirem...   \n",
       "1  the phantom of the opera the phantom of the op...   \n",
       "2  battleship potemkin scenario and script by ser...   \n",
       "3  the lost world jurassic park screenplay by dav...   \n",
       "4  metropolis by corey mandell fade in ext manhat...   \n",
       "\n",
       "                                       script_tokens  \n",
       "0  [[CLS], 1922, nos, ##fera, ##tu, cast, count, ...  \n",
       "1  [[CLS], the, phantom, of, the, opera, the, pha...  \n",
       "2  [[CLS], battleship, pot, ##em, ##kin, scenario...  \n",
       "3  [[CLS], the, lost, world, jurassic, park, scre...  \n",
       "4  [[CLS], metropolis, by, corey, man, ##dell, fa...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the tokenizer (use any pre-trained tokenizer, e.g., BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize each row in the 'script' column\n",
    "data[\"script_tokens\"] = data[\"script\"].apply( \n",
    "    lambda x: tokenizer.tokenize(x, add_special_tokens=True, max_length=500, truncation = True)) # only takes the first 500 tokens\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking\n",
    "len(data['script_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trying to feed the tokens of one script to distilbert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_script_train = train_data.iloc[0]\n",
    "one_script['script_tokens']\n",
    "\n",
    "one_script_test = test_data.iloc[0]\n",
    "one_script_val = val_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                                              Dog Day Afternoon\n",
       "rating                                                             3\n",
       "imdbid                                                         72890\n",
       "year                                                            1975\n",
       "id                                                              3799\n",
       "passed_bechdel                                                     1\n",
       "script_filename                        Dog Day Afternoon_0072890.txt\n",
       "script             dog day afternoon by frank pierson final draft...\n",
       "script_tokens      ['[CLS]', 'dog', 'day', 'afternoon', 'by', 'fr...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_script_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ucloud/.local/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7442/1115465922.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2472\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2470\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2471\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2472\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2474\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:5131\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5131\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5133\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/accelerate/data_loader.py:552\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer_utils.py:848\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    847\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3299\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[1;32m   3296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[1;32m   3297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3299\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[43mencoded_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3300\u001b[0m     )\n\u001b[1;32m   3302\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Prepare data collator for padding sequences\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Define Trainer object for training the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=one_script_train['script_tokens'],\n",
    "    eval_dataset=one_script_test['script_tokens'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "trainer.save_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Embedding the data\n",
    "\n",
    "Because we are using a pre-trained model and tokenizer, we need to first get the token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175     [101, 3899, 2154, 5027, 2011, 3581, 16067, 223...\n",
      "629     [101, 8942, 5863, 1015, 20014, 7151, 27454, 89...\n",
      "1105    [101, 2689, 2989, 1037, 2995, 2466, 2517, 2011...\n",
      "1740    [101, 2005, 2115, 9584, 21198, 5151, 2434, 900...\n",
      "1013    [101, 1996, 3035, 2517, 2011, 2848, 5253, 1015...\n",
      "Name: script_token_ids, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained tokenizer (e.g., BERT tokenizer)\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to convert words to token IDs using the tokenizer\n",
    "def words_to_token_ids(words):\n",
    "    # Tokenize the words into token IDs\n",
    "    return tokenizer.convert_tokens_to_ids(words)\n",
    "\n",
    "# Apply the function to your tokenized column (assuming `script_tokens` contains words)\n",
    "train_data[\"script_token_ids\"] = train_data[\"script_tokens\"].apply(words_to_token_ids)\n",
    "val_data[\"script_token_ids\"] = val_data[\"script_tokens\"].apply(words_to_token_ids)\n",
    "test_data[\"script_token_ids\"] = test_data[\"script_tokens\"].apply(words_to_token_ids)\n",
    "\n",
    "# Check the result\n",
    "print(train_data[\"script_token_ids\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>script_tokens</th>\n",
       "      <th>script_token_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Dog Day Afternoon</td>\n",
       "      <td>3</td>\n",
       "      <td>72890</td>\n",
       "      <td>1975</td>\n",
       "      <td>3799</td>\n",
       "      <td>1</td>\n",
       "      <td>Dog Day Afternoon_0072890.txt</td>\n",
       "      <td>dog day afternoon by frank pierson final draft...</td>\n",
       "      <td>[[CLS], dog, day, afternoon, by, frank, piers,...</td>\n",
       "      <td>[101, 3899, 2154, 5027, 2011, 3581, 16067, 223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Batman &amp;amp; Robin</td>\n",
       "      <td>2</td>\n",
       "      <td>118688</td>\n",
       "      <td>1997</td>\n",
       "      <td>1026</td>\n",
       "      <td>0</td>\n",
       "      <td>Batman Robin_0118688.txt</td>\n",
       "      <td>batman robin 1 int batcave batmans costume vau...</td>\n",
       "      <td>[[CLS], batman, robin, 1, int, bat, ##cave, ba...</td>\n",
       "      <td>[101, 8942, 5863, 1015, 20014, 7151, 27454, 89...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>Changeling</td>\n",
       "      <td>3</td>\n",
       "      <td>824747</td>\n",
       "      <td>2008</td>\n",
       "      <td>286</td>\n",
       "      <td>1</td>\n",
       "      <td>Changeling_0824747.txt</td>\n",
       "      <td>changeling a true story written by j michael s...</td>\n",
       "      <td>[[CLS], change, ##ling, a, true, story, writte...</td>\n",
       "      <td>[101, 2689, 2989, 1037, 2995, 2466, 2517, 2011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>Gisaengchung</td>\n",
       "      <td>3</td>\n",
       "      <td>6751668</td>\n",
       "      <td>2019</td>\n",
       "      <td>8768</td>\n",
       "      <td>1</td>\n",
       "      <td>Parasite_6751668.txt</td>\n",
       "      <td>for your consideration parasite outstanding or...</td>\n",
       "      <td>[[CLS], for, your, consideration, parasite, ou...</td>\n",
       "      <td>[101, 2005, 2115, 9584, 21198, 5151, 2434, 900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>Queen, The</td>\n",
       "      <td>3</td>\n",
       "      <td>436697</td>\n",
       "      <td>2006</td>\n",
       "      <td>1412</td>\n",
       "      <td>1</td>\n",
       "      <td>The Queen_0436697.txt</td>\n",
       "      <td>the queen written by peter morgan 1 archive te...</td>\n",
       "      <td>[[CLS], the, queen, written, by, peter, morgan...</td>\n",
       "      <td>[101, 1996, 3035, 2517, 2011, 2848, 5253, 1015...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title  rating   imdbid  year    id  passed_bechdel  \\\n",
       "175    Dog Day Afternoon       3    72890  1975  3799               1   \n",
       "629   Batman &amp; Robin       2   118688  1997  1026               0   \n",
       "1105          Changeling       3   824747  2008   286               1   \n",
       "1740        Gisaengchung       3  6751668  2019  8768               1   \n",
       "1013          Queen, The       3   436697  2006  1412               1   \n",
       "\n",
       "                    script_filename  \\\n",
       "175   Dog Day Afternoon_0072890.txt   \n",
       "629        Batman Robin_0118688.txt   \n",
       "1105         Changeling_0824747.txt   \n",
       "1740           Parasite_6751668.txt   \n",
       "1013          The Queen_0436697.txt   \n",
       "\n",
       "                                                 script  \\\n",
       "175   dog day afternoon by frank pierson final draft...   \n",
       "629   batman robin 1 int batcave batmans costume vau...   \n",
       "1105  changeling a true story written by j michael s...   \n",
       "1740  for your consideration parasite outstanding or...   \n",
       "1013  the queen written by peter morgan 1 archive te...   \n",
       "\n",
       "                                          script_tokens  \\\n",
       "175   [[CLS], dog, day, afternoon, by, frank, piers,...   \n",
       "629   [[CLS], batman, robin, 1, int, bat, ##cave, ba...   \n",
       "1105  [[CLS], change, ##ling, a, true, story, writte...   \n",
       "1740  [[CLS], for, your, consideration, parasite, ou...   \n",
       "1013  [[CLS], the, queen, written, by, peter, morgan...   \n",
       "\n",
       "                                       script_token_ids  \n",
       "175   [101, 3899, 2154, 5027, 2011, 3581, 16067, 223...  \n",
       "629   [101, 8942, 5863, 1015, 20014, 7151, 27454, 89...  \n",
       "1105  [101, 2689, 2989, 1037, 2995, 2466, 2517, 2011...  \n",
       "1740  [101, 2005, 2115, 9584, 21198, 5151, 2434, 900...  \n",
       "1013  [101, 1996, 3035, 2517, 2011, 2848, 5253, 1015...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175     [tensor(-0.4918), tensor(0.2709), tensor(0.378...\n",
      "629     [tensor(-0.9178), tensor(0.4156), tensor(-0.00...\n",
      "1105    [tensor(-0.2787), tensor(-0.0085), tensor(0.75...\n",
      "1740    [tensor(-0.3097), tensor(0.3638), tensor(0.188...\n",
      "1013    [tensor(-0.4474), tensor(0.0109), tensor(0.446...\n",
      "Name: embeddings, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# embedding the script token ids\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "#model_name = 'bert-base-uncased'\n",
    "#model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to obtain embeddings for token IDs\n",
    "def get_embeddings(token_ids):\n",
    "    # Convert list of token IDs to a tensor\n",
    "    tokens_tensor = torch.tensor([token_ids])  # Add a batch dimension\n",
    "    \n",
    "    # Create an attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = (tokens_tensor != tokenizer.pad_token_id).int()\n",
    "    \n",
    "    # Pass through the model (no gradient computation to save memory)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=tokens_tensor, attention_mask=attention_mask)\n",
    "    \n",
    "    # Extract the embeddings (usually from the last hidden layer)\n",
    "    # The shape of outputs.last_hidden_state is (batch_size, sequence_length, hidden_size)\n",
    "    # We usually take the embeddings of the [CLS] token (index 0)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]  # [CLS] token's embedding\n",
    "\n",
    "    return cls_embedding\n",
    "\n",
    "# Apply the function to the tokenized column\n",
    "train_data[\"embeddings\"] = train_data[\"script_token_ids\"].apply(get_embeddings)\n",
    "val_data[\"embeddings\"] = val_data[\"script_token_ids\"].apply(get_embeddings)\n",
    "test_data[\"embeddings\"] = test_data[\"script_token_ids\"].apply(get_embeddings)\n",
    "\n",
    "# Check the result\n",
    "print(train_data[\"embeddings\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "train_data.to_csv(\"train_embedded.csv\", index= False)\n",
    "test_data.to_csv(\"test_embedded.csv\", index= False)\n",
    "val_data.to_csv(\"validation_embedded.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>script_tokens</th>\n",
       "      <th>script_token_ids</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Dog Day Afternoon</td>\n",
       "      <td>3</td>\n",
       "      <td>72890</td>\n",
       "      <td>1975</td>\n",
       "      <td>3799</td>\n",
       "      <td>1</td>\n",
       "      <td>Dog Day Afternoon_0072890.txt</td>\n",
       "      <td>dog day afternoon by frank pierson final draft...</td>\n",
       "      <td>[[CLS], dog, day, afternoon, by, frank, piers,...</td>\n",
       "      <td>[101, 3899, 2154, 5027, 2011, 3581, 16067, 223...</td>\n",
       "      <td>[tensor(-0.4918), tensor(0.2709), tensor(0.378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>Batman &amp;amp; Robin</td>\n",
       "      <td>2</td>\n",
       "      <td>118688</td>\n",
       "      <td>1997</td>\n",
       "      <td>1026</td>\n",
       "      <td>0</td>\n",
       "      <td>Batman Robin_0118688.txt</td>\n",
       "      <td>batman robin 1 int batcave batmans costume vau...</td>\n",
       "      <td>[[CLS], batman, robin, 1, int, bat, ##cave, ba...</td>\n",
       "      <td>[101, 8942, 5863, 1015, 20014, 7151, 27454, 89...</td>\n",
       "      <td>[tensor(-0.9178), tensor(0.4156), tensor(-0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>Changeling</td>\n",
       "      <td>3</td>\n",
       "      <td>824747</td>\n",
       "      <td>2008</td>\n",
       "      <td>286</td>\n",
       "      <td>1</td>\n",
       "      <td>Changeling_0824747.txt</td>\n",
       "      <td>changeling a true story written by j michael s...</td>\n",
       "      <td>[[CLS], change, ##ling, a, true, story, writte...</td>\n",
       "      <td>[101, 2689, 2989, 1037, 2995, 2466, 2517, 2011...</td>\n",
       "      <td>[tensor(-0.2787), tensor(-0.0085), tensor(0.75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>Gisaengchung</td>\n",
       "      <td>3</td>\n",
       "      <td>6751668</td>\n",
       "      <td>2019</td>\n",
       "      <td>8768</td>\n",
       "      <td>1</td>\n",
       "      <td>Parasite_6751668.txt</td>\n",
       "      <td>for your consideration parasite outstanding or...</td>\n",
       "      <td>[[CLS], for, your, consideration, parasite, ou...</td>\n",
       "      <td>[101, 2005, 2115, 9584, 21198, 5151, 2434, 900...</td>\n",
       "      <td>[tensor(-0.3097), tensor(0.3638), tensor(0.188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>Queen, The</td>\n",
       "      <td>3</td>\n",
       "      <td>436697</td>\n",
       "      <td>2006</td>\n",
       "      <td>1412</td>\n",
       "      <td>1</td>\n",
       "      <td>The Queen_0436697.txt</td>\n",
       "      <td>the queen written by peter morgan 1 archive te...</td>\n",
       "      <td>[[CLS], the, queen, written, by, peter, morgan...</td>\n",
       "      <td>[101, 1996, 3035, 2517, 2011, 2848, 5253, 1015...</td>\n",
       "      <td>[tensor(-0.4474), tensor(0.0109), tensor(0.446...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title  rating   imdbid  year    id  passed_bechdel  \\\n",
       "175    Dog Day Afternoon       3    72890  1975  3799               1   \n",
       "629   Batman &amp; Robin       2   118688  1997  1026               0   \n",
       "1105          Changeling       3   824747  2008   286               1   \n",
       "1740        Gisaengchung       3  6751668  2019  8768               1   \n",
       "1013          Queen, The       3   436697  2006  1412               1   \n",
       "\n",
       "                    script_filename  \\\n",
       "175   Dog Day Afternoon_0072890.txt   \n",
       "629        Batman Robin_0118688.txt   \n",
       "1105         Changeling_0824747.txt   \n",
       "1740           Parasite_6751668.txt   \n",
       "1013          The Queen_0436697.txt   \n",
       "\n",
       "                                                 script  \\\n",
       "175   dog day afternoon by frank pierson final draft...   \n",
       "629   batman robin 1 int batcave batmans costume vau...   \n",
       "1105  changeling a true story written by j michael s...   \n",
       "1740  for your consideration parasite outstanding or...   \n",
       "1013  the queen written by peter morgan 1 archive te...   \n",
       "\n",
       "                                          script_tokens  \\\n",
       "175   [[CLS], dog, day, afternoon, by, frank, piers,...   \n",
       "629   [[CLS], batman, robin, 1, int, bat, ##cave, ba...   \n",
       "1105  [[CLS], change, ##ling, a, true, story, writte...   \n",
       "1740  [[CLS], for, your, consideration, parasite, ou...   \n",
       "1013  [[CLS], the, queen, written, by, peter, morgan...   \n",
       "\n",
       "                                       script_token_ids  \\\n",
       "175   [101, 3899, 2154, 5027, 2011, 3581, 16067, 223...   \n",
       "629   [101, 8942, 5863, 1015, 20014, 7151, 27454, 89...   \n",
       "1105  [101, 2689, 2989, 1037, 2995, 2466, 2517, 2011...   \n",
       "1740  [101, 2005, 2115, 9584, 21198, 5151, 2434, 900...   \n",
       "1013  [101, 1996, 3035, 2517, 2011, 2848, 5253, 1015...   \n",
       "\n",
       "                                             embeddings  \n",
       "175   [tensor(-0.4918), tensor(0.2709), tensor(0.378...  \n",
       "629   [tensor(-0.9178), tensor(0.4156), tensor(-0.00...  \n",
       "1105  [tensor(-0.2787), tensor(-0.0085), tensor(0.75...  \n",
       "1740  [tensor(-0.3097), tensor(0.3638), tensor(0.188...  \n",
       "1013  [tensor(-0.4474), tensor(0.0109), tensor(0.446...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175     [tensor(-0.4918), tensor(0.2709), tensor(0.378...\n",
       "629     [tensor(-0.9178), tensor(0.4156), tensor(-0.00...\n",
       "1105    [tensor(-0.2787), tensor(-0.0085), tensor(0.75...\n",
       "1740    [tensor(-0.3097), tensor(0.3638), tensor(0.188...\n",
       "1013    [tensor(-0.4474), tensor(0.0109), tensor(0.446...\n",
       "                              ...                        \n",
       "1130    [tensor(0.0016), tensor(0.2415), tensor(0.6381...\n",
       "1294    [tensor(-0.0257), tensor(0.1727), tensor(0.732...\n",
       "860     [tensor(-0.8456), tensor(0.1440), tensor(0.049...\n",
       "1459    [tensor(-0.5754), tensor(0.3761), tensor(0.309...\n",
       "1126    [tensor(-0.5921), tensor(0.1790), tensor(0.261...\n",
       "Name: embeddings, Length: 1424, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>script_tokens</th>\n",
       "      <th>script_token_ids</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog Day Afternoon</td>\n",
       "      <td>3</td>\n",
       "      <td>72890</td>\n",
       "      <td>1975</td>\n",
       "      <td>3799</td>\n",
       "      <td>1</td>\n",
       "      <td>Dog Day Afternoon_0072890.txt</td>\n",
       "      <td>dog day afternoon by frank pierson final draft...</td>\n",
       "      <td>['[CLS]', 'dog', 'day', 'afternoon', 'by', 'fr...</td>\n",
       "      <td>[101, 3899, 2154, 5027, 2011, 3581, 16067, 223...</td>\n",
       "      <td>tensor([-4.9176e-01,  2.7094e-01,  3.7832e-01,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Batman &amp;amp; Robin</td>\n",
       "      <td>2</td>\n",
       "      <td>118688</td>\n",
       "      <td>1997</td>\n",
       "      <td>1026</td>\n",
       "      <td>0</td>\n",
       "      <td>Batman Robin_0118688.txt</td>\n",
       "      <td>batman robin 1 int batcave batmans costume vau...</td>\n",
       "      <td>['[CLS]', 'batman', 'robin', '1', 'int', 'bat'...</td>\n",
       "      <td>[101, 8942, 5863, 1015, 20014, 7151, 27454, 89...</td>\n",
       "      <td>tensor([-9.1781e-01,  4.1560e-01, -8.7145e-03,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changeling</td>\n",
       "      <td>3</td>\n",
       "      <td>824747</td>\n",
       "      <td>2008</td>\n",
       "      <td>286</td>\n",
       "      <td>1</td>\n",
       "      <td>Changeling_0824747.txt</td>\n",
       "      <td>changeling a true story written by j michael s...</td>\n",
       "      <td>['[CLS]', 'change', '##ling', 'a', 'true', 'st...</td>\n",
       "      <td>[101, 2689, 2989, 1037, 2995, 2466, 2517, 2011...</td>\n",
       "      <td>tensor([-2.7869e-01, -8.5252e-03,  7.5140e-01,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gisaengchung</td>\n",
       "      <td>3</td>\n",
       "      <td>6751668</td>\n",
       "      <td>2019</td>\n",
       "      <td>8768</td>\n",
       "      <td>1</td>\n",
       "      <td>Parasite_6751668.txt</td>\n",
       "      <td>for your consideration parasite outstanding or...</td>\n",
       "      <td>['[CLS]', 'for', 'your', 'consideration', 'par...</td>\n",
       "      <td>[101, 2005, 2115, 9584, 21198, 5151, 2434, 900...</td>\n",
       "      <td>tensor([-3.0974e-01,  3.6384e-01,  1.8883e-01,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queen, The</td>\n",
       "      <td>3</td>\n",
       "      <td>436697</td>\n",
       "      <td>2006</td>\n",
       "      <td>1412</td>\n",
       "      <td>1</td>\n",
       "      <td>The Queen_0436697.txt</td>\n",
       "      <td>the queen written by peter morgan 1 archive te...</td>\n",
       "      <td>['[CLS]', 'the', 'queen', 'written', 'by', 'pe...</td>\n",
       "      <td>[101, 1996, 3035, 2517, 2011, 2848, 5253, 1015...</td>\n",
       "      <td>tensor([-4.4745e-01,  1.0884e-02,  4.4611e-01,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title  rating   imdbid  year    id  passed_bechdel  \\\n",
       "0   Dog Day Afternoon       3    72890  1975  3799               1   \n",
       "1  Batman &amp; Robin       2   118688  1997  1026               0   \n",
       "2          Changeling       3   824747  2008   286               1   \n",
       "3        Gisaengchung       3  6751668  2019  8768               1   \n",
       "4          Queen, The       3   436697  2006  1412               1   \n",
       "\n",
       "                 script_filename  \\\n",
       "0  Dog Day Afternoon_0072890.txt   \n",
       "1       Batman Robin_0118688.txt   \n",
       "2         Changeling_0824747.txt   \n",
       "3           Parasite_6751668.txt   \n",
       "4          The Queen_0436697.txt   \n",
       "\n",
       "                                              script  \\\n",
       "0  dog day afternoon by frank pierson final draft...   \n",
       "1  batman robin 1 int batcave batmans costume vau...   \n",
       "2  changeling a true story written by j michael s...   \n",
       "3  for your consideration parasite outstanding or...   \n",
       "4  the queen written by peter morgan 1 archive te...   \n",
       "\n",
       "                                       script_tokens  \\\n",
       "0  ['[CLS]', 'dog', 'day', 'afternoon', 'by', 'fr...   \n",
       "1  ['[CLS]', 'batman', 'robin', '1', 'int', 'bat'...   \n",
       "2  ['[CLS]', 'change', '##ling', 'a', 'true', 'st...   \n",
       "3  ['[CLS]', 'for', 'your', 'consideration', 'par...   \n",
       "4  ['[CLS]', 'the', 'queen', 'written', 'by', 'pe...   \n",
       "\n",
       "                                    script_token_ids  \\\n",
       "0  [101, 3899, 2154, 5027, 2011, 3581, 16067, 223...   \n",
       "1  [101, 8942, 5863, 1015, 20014, 7151, 27454, 89...   \n",
       "2  [101, 2689, 2989, 1037, 2995, 2466, 2517, 2011...   \n",
       "3  [101, 2005, 2115, 9584, 21198, 5151, 2434, 900...   \n",
       "4  [101, 1996, 3035, 2517, 2011, 2848, 5253, 1015...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  tensor([-4.9176e-01,  2.7094e-01,  3.7832e-01,...  \n",
       "1  tensor([-9.1781e-01,  4.1560e-01, -8.7145e-03,...  \n",
       "2  tensor([-2.7869e-01, -8.5252e-03,  7.5140e-01,...  \n",
       "3  tensor([-3.0974e-01,  3.6384e-01,  1.8883e-01,...  \n",
       "4  tensor([-4.4745e-01,  1.0884e-02,  4.4611e-01,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train_embedded.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                                               Dog Day Afternoon\n",
       "rating                                                              3\n",
       "imdbid                                                          72890\n",
       "year                                                             1975\n",
       "id                                                               3799\n",
       "passed_bechdel                                                      1\n",
       "script_filename                         Dog Day Afternoon_0072890.txt\n",
       "script              dog day afternoon by frank pierson final draft...\n",
       "script_tokens       ['[CLS]', 'dog', 'day', 'afternoon', 'by', 'fr...\n",
       "script_token_ids    [101, 3899, 2154, 5027, 2011, 3581, 16067, 223...\n",
       "embeddings          tensor([-4.9176e-01,  2.7094e-01,  3.7832e-01,...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.9176e-01,  2.7094e-01,  3.7832e-01,  1.4730e-01, -6.1553e-01,\n",
      "        -2.6600e-01,  9.7452e-02,  5.9233e-01, -2.2108e-01, -2.0372e-01,\n",
      "         2.8129e-01, -2.2023e-01, -5.0598e-01,  1.4258e-01,  4.2681e-01,\n",
      "         7.3866e-01,  2.3089e-01,  1.2726e-01,  3.4154e-01,  7.3222e-01,\n",
      "         8.1984e-02, -5.5034e-01,  4.4979e-01,  4.6505e-02,  7.8282e-01,\n",
      "        -3.2723e-01, -1.3330e-01, -2.6965e-01,  4.0749e-02,  3.9928e-01,\n",
      "         1.3295e-01,  5.9182e-01,  4.2363e-01, -6.2712e-01,  5.2497e-01,\n",
      "        -8.3174e-01,  6.9395e-01, -1.6846e-01,  6.6972e-01,  2.3508e-01,\n",
      "        -4.8936e-01,  4.9967e-01,  5.0280e-02, -4.3389e-01, -3.4167e-01,\n",
      "        -4.8004e-02, -3.6919e+00,  7.3405e-03,  4.9712e-01, -6.8412e-01,\n",
      "         5.0101e-01, -1.0929e+00,  1.3477e-01,  5.6389e-01,  3.3538e-01,\n",
      "         7.5621e-01, -6.6563e-01,  2.6791e-01,  6.9026e-01, -3.5565e-01,\n",
      "         8.0729e-01, -8.3422e-01, -3.4990e-01, -8.5518e-02, -2.1785e-01,\n",
      "         5.1623e-01,  6.6995e-02,  8.8437e-02, -7.1384e-01, -9.0617e-02,\n",
      "        -4.3394e-01, -1.1330e-01,  3.7052e-01, -2.7052e-01,  5.1622e-01,\n",
      "        -2.2388e-01, -3.4704e-01,  3.5719e-01, -2.9789e-01, -2.4418e-01,\n",
      "         4.8202e-02,  3.2309e-01, -3.8717e-01,  2.5040e-01, -3.0328e-01,\n",
      "         4.5569e-01, -2.5301e-02, -1.5139e-01, -5.3413e-01,  7.8043e-01,\n",
      "         3.1053e-01, -3.8298e-01, -8.5984e-02, -1.3490e-01,  4.0529e-01,\n",
      "         7.7096e-02,  4.5968e-01,  8.8946e-01, -8.2108e-02,  5.9451e-01,\n",
      "        -1.4286e-01,  2.6152e-01,  1.8005e-01, -5.4903e-01, -3.6184e-01,\n",
      "         8.8546e-02,  6.5909e-01,  3.4568e-01,  6.5159e-01, -1.4571e+00,\n",
      "         2.8688e-01, -9.0430e-03, -3.3401e-01, -6.2855e-01,  5.5073e-02,\n",
      "        -4.3206e-01,  4.9655e-01, -7.1308e-01,  3.2324e-01,  2.8873e-01,\n",
      "        -6.0232e-01,  3.4273e-01,  4.8563e-02,  2.6730e-02,  1.4613e-01,\n",
      "         1.3861e-01, -3.1681e-01, -1.7340e-01,  5.0660e-01,  6.4439e-01,\n",
      "         2.0828e-01,  1.6906e-02,  3.5454e-02, -2.3521e-01, -5.8694e-01,\n",
      "         1.7178e-02,  3.5577e-01, -2.2236e-01, -5.1189e-01,  1.6764e-01,\n",
      "        -4.0365e-01, -2.3130e-01, -2.3037e+00,  1.2560e-01,  4.9667e-02,\n",
      "        -3.7525e-01, -1.8191e-01,  3.0326e-01, -8.5528e-02,  4.2658e-01,\n",
      "         1.9939e-01, -9.0197e-01,  4.8749e-01,  4.3944e-02, -2.1321e-01,\n",
      "         1.4404e-01, -2.8209e-01, -5.1725e-01,  2.8372e-02,  4.6968e-01,\n",
      "         3.4662e-01,  8.8744e-01, -3.3626e-02, -7.6199e-02,  4.2257e-01,\n",
      "        -3.7584e-01, -1.6028e-01,  2.5077e-01, -2.6142e-01, -4.8637e-02,\n",
      "         2.1845e-01,  6.1874e-01,  1.0865e+00, -3.6029e-01, -3.3119e-01,\n",
      "         2.2650e-01,  3.4602e-02,  3.2943e-01,  1.7875e-01, -3.4456e-01,\n",
      "        -6.9063e-01, -3.3849e-01, -1.7426e-01,  5.7119e-01, -1.1298e-01,\n",
      "        -2.9224e-02,  1.1405e-01, -3.2503e-02, -8.9749e-02,  1.7847e-01,\n",
      "         6.5884e-02, -6.3725e-01,  5.5398e-01,  2.8095e-01,  7.4669e-01,\n",
      "         4.7192e-02,  4.7280e-01, -9.5260e-01,  2.6901e-01, -7.8829e-01,\n",
      "         3.9787e-01, -4.0414e-01,  1.9236e-01,  2.2450e-01, -1.1240e+00,\n",
      "         3.0902e+00,  4.5339e-01, -5.3454e-02,  8.5401e-02,  7.0908e-01,\n",
      "        -4.3010e-01,  2.8707e-01, -5.8679e-01, -3.7069e-02, -2.0683e-01,\n",
      "         3.8809e-01,  1.8634e-01, -2.4144e-01,  1.7745e-01,  1.0959e-01,\n",
      "        -5.9002e-01,  7.8426e-01, -2.6737e-01,  2.4103e-01, -1.4060e-01,\n",
      "         1.0351e-01, -3.0847e-01, -8.9214e-01,  1.0460e-01, -1.5267e+00,\n",
      "         7.4137e-01, -1.1734e+00, -3.6886e-01,  2.2189e-02, -3.3103e-02,\n",
      "        -4.4745e-01,  2.8986e-01, -6.5667e-02,  1.4644e-01,  1.2190e-01,\n",
      "         1.1869e-01,  6.4553e-01,  3.7307e-01, -4.5520e-01, -7.4790e-02,\n",
      "         1.8561e-01,  5.7385e-01, -6.5936e-01,  5.4455e-01, -1.1876e-01,\n",
      "         2.1330e-01, -1.4710e-02,  3.4238e-01, -5.6281e-01,  1.3228e-01,\n",
      "         1.6374e-01, -3.1306e-01,  1.3612e-01, -1.9502e-01, -1.3827e-01,\n",
      "        -1.8929e-01,  2.2473e-02,  2.8110e-01,  3.1560e-01, -3.4531e-01,\n",
      "        -4.8519e-01, -4.0336e-01, -2.4722e-01, -3.5021e-01, -1.7205e-01,\n",
      "         9.1919e-01,  5.1585e-01, -4.1214e-01, -2.0692e+00,  2.8981e-01,\n",
      "        -5.7728e-02,  6.6294e-01,  1.0061e-01,  9.0250e-03, -3.6332e-01,\n",
      "         2.5314e-01,  5.9703e-01, -5.0063e-01,  5.4493e-01, -7.4349e-03,\n",
      "        -3.9979e-01,  1.1036e-01,  3.6988e-02,  1.9784e-01,  3.8679e-01,\n",
      "         9.8806e-02,  5.7073e-02,  3.5205e-01,  6.4663e-02,  9.3233e-02,\n",
      "        -2.6597e-01,  8.5617e-02,  3.7267e-01,  1.3417e-01, -3.0368e-01,\n",
      "        -1.1450e+00,  8.6831e-01, -2.8798e-01,  6.0071e-01, -1.4156e-01,\n",
      "        -1.9621e-02, -4.5474e-01, -1.6277e-01, -3.9045e+00,  1.1376e+00,\n",
      "        -2.2952e-01, -1.0324e+00, -4.4912e-01, -2.1658e-01,  5.5639e-01,\n",
      "         2.8904e-02, -6.8309e-01, -7.7550e-01,  3.2771e-01,  1.3775e-01,\n",
      "         7.2290e-03, -3.7677e-01,  7.6921e-01,  7.0748e-01,  4.4979e-01,\n",
      "        -5.3012e-01, -1.5004e-01,  1.2506e-01, -3.1970e-01, -4.9522e-01,\n",
      "         4.5838e-01, -3.3070e-01,  1.7806e-01,  3.6626e-01, -8.9412e-01,\n",
      "        -1.7406e-01, -2.7337e-01,  2.7442e-01,  4.4392e-01,  5.2094e-01,\n",
      "         1.3930e-01, -4.8652e-02, -3.5015e-01, -8.5750e-01,  8.0801e-02,\n",
      "        -4.1952e-01,  1.7536e-01, -3.3275e-01,  5.0079e-01,  4.9222e-01,\n",
      "         2.3479e-01, -2.0663e-01,  4.9562e-01, -4.0294e-01,  4.1308e-01,\n",
      "        -5.2303e-01,  3.4522e-01,  7.3818e-01,  5.1789e-02,  1.4430e-01,\n",
      "         1.1391e-01, -5.9367e-02, -4.8118e-01,  2.4648e-01,  9.1605e-01,\n",
      "         2.3796e-01,  2.4974e-01, -2.3254e-01,  8.1300e-01, -6.4834e-01,\n",
      "        -3.5807e-01,  1.7953e-01,  4.1186e-01, -2.3087e-01,  8.9284e-02,\n",
      "        -6.0644e-02, -7.9443e-02,  5.5312e-01, -1.6152e-01,  8.8537e-01,\n",
      "        -5.4640e-01, -1.9572e+00, -2.6245e-01,  3.0343e-01,  2.9718e-02,\n",
      "         3.5322e-01,  8.8880e-01,  2.4816e-02, -1.0441e-01, -3.5074e-01,\n",
      "         3.7504e-01,  1.1824e-01, -6.4596e-01,  4.8160e-02, -3.6223e-01,\n",
      "        -1.3420e-01, -7.4899e-01, -6.4270e-01, -1.8456e-01,  6.6169e-01,\n",
      "         4.2051e-01,  4.4839e-01,  5.0054e-01,  4.8179e-02,  5.9176e-01,\n",
      "        -9.1541e-01,  5.5530e-01, -2.2549e-01,  5.9295e-01,  3.3839e-03,\n",
      "        -3.5916e-02, -2.4143e-01, -3.1376e-01, -4.7637e-01, -9.1192e-01,\n",
      "         9.6715e-01, -2.7040e-01,  2.1710e-01, -1.3980e-02, -3.7235e-01,\n",
      "        -1.0343e-02,  1.2879e-01,  6.0733e-01, -3.6130e-01, -6.3851e-02,\n",
      "         1.1666e+00,  3.8663e-01, -4.4692e-01,  2.6517e-01, -1.1674e-01,\n",
      "         7.6062e-02, -1.7799e-01, -8.7526e-01, -1.5076e-01, -4.0900e-01,\n",
      "        -7.1348e-02, -3.8568e-01,  7.2362e-01, -8.4217e-02, -8.3852e-02,\n",
      "        -1.1528e+00, -4.5320e-01, -1.6239e-01,  1.1363e-01,  6.3004e-02,\n",
      "         1.3158e-01,  7.0680e-01, -4.0971e-01,  1.1124e-01,  3.1520e-03,\n",
      "        -6.1802e-01,  4.6113e-01, -8.4955e-01,  1.1283e+00, -2.0305e-01,\n",
      "        -6.7686e-02, -5.5417e-01,  3.5767e-02, -4.8283e-01,  1.6023e-01,\n",
      "        -4.7389e-02, -6.8166e-01,  4.2435e-01, -4.5376e-01, -7.6629e-01,\n",
      "         6.8734e-02, -7.7977e-01, -3.4393e-01, -4.1188e-02,  9.0453e-01,\n",
      "        -2.4003e+00, -2.7125e-01,  4.5923e-01, -7.6614e-02,  5.7311e-01,\n",
      "        -1.0441e+00, -1.0292e-01,  5.9171e-01, -5.4748e-01, -7.7383e-01,\n",
      "        -5.8754e-01, -1.2211e-01, -6.5855e-01,  4.9783e-01,  4.6218e-01,\n",
      "        -1.6823e-01,  1.2975e-02, -6.1210e-01, -2.1804e-01,  4.8759e-01,\n",
      "        -6.1701e-01,  5.3826e-01,  3.6275e-01,  2.4380e-01, -2.0298e-01,\n",
      "        -2.4376e-01, -1.9888e-01,  7.2511e-01, -1.3620e-01,  6.9849e-02,\n",
      "         4.8712e-02, -3.6478e-01, -2.3922e-01,  4.9415e-01,  7.6967e-01,\n",
      "         4.5098e-01,  3.3715e-01, -2.4189e-02,  5.6087e-01, -2.9381e-02,\n",
      "        -4.9276e-01,  4.5394e-01,  1.5416e-01,  5.0812e-01,  7.1897e-01,\n",
      "         3.1895e-01,  5.2606e-02, -1.8391e-01,  5.6380e-01, -2.5065e-01,\n",
      "         2.0112e-01, -1.5501e-01, -2.8951e-01, -7.4064e-01,  1.1357e-02,\n",
      "        -5.8204e-02, -7.4931e-01,  2.0839e-01,  3.1288e-02, -5.2220e-02,\n",
      "        -1.4054e-01, -1.7681e-01, -6.2364e-01,  2.5141e-01,  4.8120e-02,\n",
      "        -7.7625e-01,  5.1595e-01, -2.0086e-01,  7.0170e-01,  6.2775e-01,\n",
      "         2.4881e-01,  2.0796e-01, -1.0851e+00,  5.9278e-01,  6.1876e-01,\n",
      "        -4.7103e-02,  6.9636e-02,  3.9423e-01,  4.3650e-01, -3.5843e-01,\n",
      "         6.1523e-01, -8.2499e-01, -2.2082e-01,  8.1153e-01,  8.7115e-02,\n",
      "        -8.7226e-02, -3.3720e-01, -3.5649e-02,  1.7004e-01,  3.3197e-01,\n",
      "        -2.0595e-01, -3.8000e-01, -2.4935e-01,  2.4209e-01, -1.2886e-01,\n",
      "         2.6247e-01,  3.1493e-01, -1.0867e-01,  1.6041e-01,  3.4858e-01,\n",
      "        -1.0393e+00, -1.6321e-01,  5.1232e-01,  1.7511e-01,  7.6627e-01,\n",
      "         9.2526e-01,  5.5090e-01, -2.6794e-01, -3.4354e-01,  9.6073e-02,\n",
      "         2.1350e-01,  2.2774e-01,  5.1113e-02, -5.3153e-03,  1.0286e-01,\n",
      "         1.7966e-02,  6.8683e-01, -4.6164e-01,  1.1827e+00,  1.3520e-01,\n",
      "        -4.4434e-02, -3.0193e-01,  6.3026e-01,  4.7787e-01, -2.0830e-01,\n",
      "        -1.0619e-01,  3.3594e-02,  3.9059e-01, -4.8627e-01,  2.2457e-01,\n",
      "         2.5652e-01, -1.3351e-01,  5.8032e-01,  1.8040e-01,  5.1975e-01,\n",
      "        -6.5855e-01, -1.8531e-01, -2.4958e-01, -4.1880e-01, -1.2364e-01,\n",
      "         1.6790e-01, -9.2916e-02, -5.7847e-01,  3.4403e-01,  1.6841e-01,\n",
      "        -6.8873e-01,  5.2161e-02,  5.8245e-01, -3.9247e-01, -5.3526e-01,\n",
      "         7.1580e-01,  1.1488e+00, -2.3398e-01, -7.3509e-02,  1.6408e-01,\n",
      "        -1.9764e-01, -3.5729e-01,  3.1750e-01,  1.1889e-01, -1.6143e-01,\n",
      "         9.6120e-01, -3.6650e-01, -1.9406e-01,  4.2047e-01, -1.3543e-01,\n",
      "        -6.2811e-01,  1.9356e-01,  1.0319e-01, -3.3547e-01, -5.1024e-02,\n",
      "        -6.0376e-01,  3.6564e-02,  2.6703e-01,  1.0235e-01, -1.0027e+00,\n",
      "        -8.5281e-01, -4.8568e-02,  4.7723e-02, -4.5799e-01,  1.0433e-01,\n",
      "         4.3271e-01, -2.1416e-01, -3.2281e-01, -5.8141e-02,  8.2644e-02,\n",
      "         3.2052e-01,  2.5173e-01, -2.4615e-01, -5.6967e-01,  6.0518e-01,\n",
      "         2.3579e-01, -1.5324e-01,  3.7026e-01,  8.4360e-01,  6.3614e-01,\n",
      "        -2.2945e-01, -3.4246e-01, -1.3792e+00,  2.5623e-01,  8.9576e-01,\n",
      "         3.1177e-01, -4.5585e-01,  2.0721e-01,  2.8592e-01,  1.9577e-01,\n",
      "        -4.9396e-01, -2.4245e-01,  2.1449e-02,  4.0886e-01,  1.2561e-01,\n",
      "        -4.4040e-01, -5.1999e-02, -3.4727e-01,  7.2357e-01, -1.7090e-01,\n",
      "        -8.7550e-01, -7.9298e-02,  6.5523e-01,  3.3646e-01, -6.0579e-01,\n",
      "        -4.2560e-01, -7.0764e-01,  5.7689e-03,  7.8823e-01, -2.6181e-01,\n",
      "         2.9186e-01,  1.6253e-02, -5.6332e-02,  7.3869e-02, -7.2796e-01,\n",
      "        -2.3674e-01,  3.5041e-02,  1.5890e-01, -6.4877e-01, -6.2495e-02,\n",
      "         1.0957e-01, -2.7272e-01, -5.2122e-01,  8.8007e-01, -1.6017e-01,\n",
      "        -4.1798e-01,  2.3658e-01,  2.2671e-01,  4.5389e-01,  2.4279e-02,\n",
      "         7.6837e-01,  1.5286e-01,  1.7485e-01,  4.0847e-01,  6.5273e-01,\n",
      "        -1.2953e-01,  1.0739e-01, -1.8224e-01,  3.0618e-01, -8.0933e-02,\n",
      "         9.8005e-02, -6.3810e-01,  8.9209e-02, -3.2674e-01, -1.0768e-01,\n",
      "        -4.1058e-02,  3.8405e-01,  3.1151e-01,  1.4908e-02, -8.4433e-02,\n",
      "         9.3389e-02, -2.7406e-01, -2.5834e-01,  1.4198e-02,  5.4756e-01,\n",
      "         1.4361e-01, -2.4161e-02, -3.4807e-01,  8.7835e-01, -4.7327e-01,\n",
      "         5.7255e-01,  7.2984e-02,  4.0257e-02,  4.5523e-01, -1.1694e-01,\n",
      "         3.4872e-01,  2.1714e-01, -3.2614e+00, -6.4493e-01, -1.9353e-01,\n",
      "         2.7062e-02,  2.6653e-01, -4.5367e-01, -6.6882e-02, -9.1446e-01,\n",
      "        -6.6459e-02, -1.3805e-01,  4.1801e-01,  3.9178e-01, -2.8529e-01,\n",
      "        -6.4153e-01,  5.8894e-01, -4.9530e-01])\n"
     ]
    }
   ],
   "source": [
    "print(train_data['embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       tensor([-4.9176e-01,  2.7094e-01,  3.7832e-01,...\n",
       "1       tensor([-9.1781e-01,  4.1560e-01, -8.7145e-03,...\n",
       "2       tensor([-2.7869e-01, -8.5252e-03,  7.5140e-01,...\n",
       "3       tensor([-3.0974e-01,  3.6384e-01,  1.8883e-01,...\n",
       "4       tensor([-4.4745e-01,  1.0884e-02,  4.4611e-01,...\n",
       "                              ...                        \n",
       "1419    tensor([ 1.6356e-03,  2.4148e-01,  6.3814e-01,...\n",
       "1420    tensor([-2.5687e-02,  1.7268e-01,  7.3295e-01,...\n",
       "1421    tensor([-8.4565e-01,  1.4402e-01,  4.9040e-02,...\n",
       "1422    tensor([-5.7536e-01,  3.7615e-01,  3.0960e-01,...\n",
       "1423    tensor([-5.9207e-01,  1.7896e-01,  2.6125e-01,...\n",
       "Name: embeddings, Length: 1424, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['embeddings'] # format is slightly different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing a small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22b6b128206445788ce7cefa9131071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d5c74ecb33491392f0f2ef418d0b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "# Load the pre-trained DistilBERT model with a classification head\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DON't know about the code below......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     19\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Move batch to the device (GPU or CPU)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     input_ids, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     23\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create DataLoader for the training set\n",
    "train_data = TensorDataset(train_embeddings, train_labels)  # Use embeddings if you already have them\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Move batch to the device (GPU or CPU)\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss after each epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
