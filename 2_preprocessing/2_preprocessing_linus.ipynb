{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by: \n",
    "\n",
    "Date: 2024-12-07 \n",
    "\n",
    "Latest change when and what:\n",
    "\n",
    "Notes:\n",
    "\n",
    "# 2. Preprocessing\n",
    "\n",
    "Cleaning the data, tokenizing it, splitting it into test, train and validation, and finally embedding the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import kagglehub\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>decade</th>\n",
       "      <th>5_year_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1307</td>\n",
       "      <td>Nosferatu, eine Symphonie des Grauens</td>\n",
       "      <td>13442</td>\n",
       "      <td>1922</td>\n",
       "      <td>0</td>\n",
       "      <td>Nosferatu_0013442.txt</td>\n",
       "      <td>\\n\\n                              1922\\n\\n\\n\\n...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1305</td>\n",
       "      <td>Phantom of the Opera, The</td>\n",
       "      <td>16220</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Phantom of the Opera_0016220.txt</td>\n",
       "      <td>The Phantom of the Opera\\n\\nTHE PHANTOM OF THE...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1308</td>\n",
       "      <td>Battleship Potemkin</td>\n",
       "      <td>15648</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>Battleship Potemkin_0015648.txt</td>\n",
       "      <td>Battleship Potemkin\\n\\nScenario and script by ...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5514</td>\n",
       "      <td>Lost World, The</td>\n",
       "      <td>16039</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Lost World_0016039.txt</td>\n",
       "      <td>THE LOST WORLD\\nJURASSIC PARK\\n\\nscreenplay by...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1267</td>\n",
       "      <td>Metropolis</td>\n",
       "      <td>17136</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>Metropolis_0017136.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  ...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating    id                                  title  imdbid  year  \\\n",
       "0       2  1307  Nosferatu, eine Symphonie des Grauens   13442  1922   \n",
       "1       2  1305              Phantom of the Opera, The   16220  1925   \n",
       "2       0  1308                    Battleship Potemkin   15648  1925   \n",
       "3       2  5514                        Lost World, The   16039  1925   \n",
       "4       1  1267                             Metropolis   17136  1927   \n",
       "\n",
       "   passed_bechdel                       script_filename  \\\n",
       "0               0                 Nosferatu_0013442.txt   \n",
       "1               0  The Phantom of the Opera_0016220.txt   \n",
       "2               0       Battleship Potemkin_0015648.txt   \n",
       "3               0            The Lost World_0016039.txt   \n",
       "4               0                Metropolis_0017136.txt   \n",
       "\n",
       "                                              script  decade  5_year_bin  \n",
       "0  \\n\\n                              1922\\n\\n\\n\\n...    1920        1920  \n",
       "1  The Phantom of the Opera\\n\\nTHE PHANTOM OF THE...    1920        1925  \n",
       "2  Battleship Potemkin\\n\\nScenario and script by ...    1920        1925  \n",
       "3  THE LOST WORLD\\nJURASSIC PARK\\n\\nscreenplay by...    1920        1925  \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  ...    1920        1925  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"../1_data_acquisition/data/labels_and_scripts.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning the data\n",
    "\n",
    "Removing '/n', lowercasing, removing special characters, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>decade</th>\n",
       "      <th>5_year_bin</th>\n",
       "      <th>script_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1307</td>\n",
       "      <td>Nosferatu, eine Symphonie des Grauens</td>\n",
       "      <td>13442</td>\n",
       "      <td>1922</td>\n",
       "      <td>0</td>\n",
       "      <td>Nosferatu_0013442.txt</td>\n",
       "      <td>1922 nosferatu cast count dracula the vampirem...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1920</td>\n",
       "      <td>[101, 4798, 16839, 27709, 8525, 3459, 4175, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1305</td>\n",
       "      <td>Phantom of the Opera, The</td>\n",
       "      <td>16220</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Phantom of the Opera_0016220.txt</td>\n",
       "      <td>the phantom of the opera the phantom of the op...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 1996, 11588, 1997, 1996, 3850, 1996, 115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1308</td>\n",
       "      <td>Battleship Potemkin</td>\n",
       "      <td>15648</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>Battleship Potemkin_0015648.txt</td>\n",
       "      <td>battleship potemkin scenario and script by ser...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 17224, 8962, 6633, 4939, 11967, 1998, 58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5514</td>\n",
       "      <td>Lost World, The</td>\n",
       "      <td>16039</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Lost World_0016039.txt</td>\n",
       "      <td>the lost world jurassic park screenplay by dav...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 1996, 2439, 2088, 19996, 2380, 9000, 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1267</td>\n",
       "      <td>Metropolis</td>\n",
       "      <td>17136</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>Metropolis_0017136.txt</td>\n",
       "      <td>metropolis by corey mandell fade in ext manhat...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 18236, 2011, 18132, 2158, 12662, 12985, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating    id                                  title  imdbid  year  \\\n",
       "0       2  1307  Nosferatu, eine Symphonie des Grauens   13442  1922   \n",
       "1       2  1305              Phantom of the Opera, The   16220  1925   \n",
       "2       0  1308                    Battleship Potemkin   15648  1925   \n",
       "3       2  5514                        Lost World, The   16039  1925   \n",
       "4       1  1267                             Metropolis   17136  1927   \n",
       "\n",
       "   passed_bechdel                       script_filename  \\\n",
       "0               0                 Nosferatu_0013442.txt   \n",
       "1               0  The Phantom of the Opera_0016220.txt   \n",
       "2               0       Battleship Potemkin_0015648.txt   \n",
       "3               0            The Lost World_0016039.txt   \n",
       "4               0                Metropolis_0017136.txt   \n",
       "\n",
       "                                              script  decade  5_year_bin  \\\n",
       "0  1922 nosferatu cast count dracula the vampirem...    1920        1920   \n",
       "1  the phantom of the opera the phantom of the op...    1920        1925   \n",
       "2  battleship potemkin scenario and script by ser...    1920        1925   \n",
       "3  the lost world jurassic park screenplay by dav...    1920        1925   \n",
       "4  metropolis by corey mandell fade in ext manhat...    1920        1925   \n",
       "\n",
       "                                       script_tokens  \n",
       "0  [101, 4798, 16839, 27709, 8525, 3459, 4175, 18...  \n",
       "1  [101, 1996, 11588, 1997, 1996, 3850, 1996, 115...  \n",
       "2  [101, 17224, 8962, 6633, 4939, 11967, 1998, 58...  \n",
       "3  [101, 1996, 2439, 2088, 19996, 2380, 9000, 201...  \n",
       "4  [101, 18236, 2011, 18132, 2158, 12662, 12985, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"script\"] = (\n",
    "    data[\"script\"]\n",
    "    .str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n",
    "    .str.replace('\\n', ' ')                   # Remove newlines\n",
    "    .str.lower()                             # Convert to lowercase\n",
    "    .str.replace(r'\\s+', ' ', regex=True)    # Replace multiple spaces with a single space\n",
    "    .str.strip()                             # Remove leading/trailing spaces\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing (500 tokens)\n",
    "\n",
    "Tokenizing using a pre-trained BERT tokenizer from transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_length=500: Specifies the maximum number of tokens to include.\n",
    "\n",
    "truncation=True: Ensures that if the text exceeds 500 tokens, it will be truncated to fit the specified length.\n",
    "\n",
    "add_special_tokens=True: Includes any special tokens required by the model, such as [CLS] and [SEP] for BERT.\n",
    "\n",
    "Transformed-based models like BERT need inputs of same length -> pad & attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>decade</th>\n",
       "      <th>5_year_bin</th>\n",
       "      <th>script_tokens</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1307</td>\n",
       "      <td>Nosferatu, eine Symphonie des Grauens</td>\n",
       "      <td>13442</td>\n",
       "      <td>1922</td>\n",
       "      <td>0</td>\n",
       "      <td>Nosferatu_0013442.txt</td>\n",
       "      <td>1922 nosferatu cast count dracula the vampirem...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1920</td>\n",
       "      <td>[101, 4798, 16839, 27709, 8525, 3459, 4175, 18...</td>\n",
       "      <td>[tensor(101), tensor(4798), tensor(16839), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1305</td>\n",
       "      <td>Phantom of the Opera, The</td>\n",
       "      <td>16220</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Phantom of the Opera_0016220.txt</td>\n",
       "      <td>the phantom of the opera the phantom of the op...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 1996, 11588, 1997, 1996, 3850, 1996, 115...</td>\n",
       "      <td>[tensor(101), tensor(1996), tensor(11588), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1308</td>\n",
       "      <td>Battleship Potemkin</td>\n",
       "      <td>15648</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>Battleship Potemkin_0015648.txt</td>\n",
       "      <td>battleship potemkin scenario and script by ser...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 17224, 8962, 6633, 4939, 11967, 1998, 58...</td>\n",
       "      <td>[tensor(101), tensor(17224), tensor(8962), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5514</td>\n",
       "      <td>Lost World, The</td>\n",
       "      <td>16039</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Lost World_0016039.txt</td>\n",
       "      <td>the lost world jurassic park screenplay by dav...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 1996, 2439, 2088, 19996, 2380, 9000, 201...</td>\n",
       "      <td>[tensor(101), tensor(1996), tensor(2439), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1267</td>\n",
       "      <td>Metropolis</td>\n",
       "      <td>17136</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>Metropolis_0017136.txt</td>\n",
       "      <td>metropolis by corey mandell fade in ext manhat...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 18236, 2011, 18132, 2158, 12662, 12985, ...</td>\n",
       "      <td>[tensor(101), tensor(18236), tensor(2011), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating    id                                  title  imdbid  year  \\\n",
       "0       2  1307  Nosferatu, eine Symphonie des Grauens   13442  1922   \n",
       "1       2  1305              Phantom of the Opera, The   16220  1925   \n",
       "2       0  1308                    Battleship Potemkin   15648  1925   \n",
       "3       2  5514                        Lost World, The   16039  1925   \n",
       "4       1  1267                             Metropolis   17136  1927   \n",
       "\n",
       "   passed_bechdel                       script_filename  \\\n",
       "0               0                 Nosferatu_0013442.txt   \n",
       "1               0  The Phantom of the Opera_0016220.txt   \n",
       "2               0       Battleship Potemkin_0015648.txt   \n",
       "3               0            The Lost World_0016039.txt   \n",
       "4               0                Metropolis_0017136.txt   \n",
       "\n",
       "                                              script  decade  5_year_bin  \\\n",
       "0  1922 nosferatu cast count dracula the vampirem...    1920        1920   \n",
       "1  the phantom of the opera the phantom of the op...    1920        1925   \n",
       "2  battleship potemkin scenario and script by ser...    1920        1925   \n",
       "3  the lost world jurassic park screenplay by dav...    1920        1925   \n",
       "4  metropolis by corey mandell fade in ext manhat...    1920        1925   \n",
       "\n",
       "                                       script_tokens  \\\n",
       "0  [101, 4798, 16839, 27709, 8525, 3459, 4175, 18...   \n",
       "1  [101, 1996, 11588, 1997, 1996, 3850, 1996, 115...   \n",
       "2  [101, 17224, 8962, 6633, 4939, 11967, 1998, 58...   \n",
       "3  [101, 1996, 2439, 2088, 19996, 2380, 9000, 201...   \n",
       "4  [101, 18236, 2011, 18132, 2158, 12662, 12985, ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [tensor(101), tensor(4798), tensor(16839), ten...   \n",
       "1  [tensor(101), tensor(1996), tensor(11588), ten...   \n",
       "2  [tensor(101), tensor(17224), tensor(8962), ten...   \n",
       "3  [tensor(101), tensor(1996), tensor(2439), tens...   \n",
       "4  [tensor(101), tensor(18236), tensor(2011), ten...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_encode(text, tokenizer, max_length=500):\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "    return encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\n",
    "# Apply to all scripts\n",
    "data[['input_ids', 'attention_mask']] = data['script'].apply(\n",
    "    lambda x: pd.Series(tokenize_and_encode(x, tokenizer, max_length=500))\n",
    ")\n",
    "\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking\n",
    "len(data['script_tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Splitting the data into test, train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1424\n",
      "Validation size: 178\n",
      "Test size: 179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Split the data into train and temp (validation+test) sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)  # 20% for validation+test\n",
    "\n",
    "# Step 2: Split temp_data into validation and test sets (10% each)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)  # 50% of temp (10% of original)\n",
    "\n",
    "# Display the sizes of each set\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the test, train and val datasets\n",
    "train_data.to_csv(\"train.csv\", index= False)\n",
    "test_data.to_csv(\"test.csv\", index= False)\n",
    "val_data.to_csv(\"validation.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1718a9e6e584e37a35d9c88e681d43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.input_ids = torch.stack(data['input_ids'].tolist())\n",
    "        self.attention_mask = torch.stack(data['attention_mask'].tolist())\n",
    "        self.labels = torch.tensor(data['passed_bechdel'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = ScriptDataset(train_data)\n",
    "val_dataset = ScriptDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.5384442210197449\n",
      "Epoch 2: Loss = 0.26806384325027466\n",
      "Epoch 3: Loss = 0.07606004923582077\n",
      "Epoch 4: Loss = 0.1101921796798706\n",
      "Epoch 5: Loss = 0.04116030037403107\n",
      "Epoch 6: Loss = 0.02917688526213169\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(6):  # 3 epochs as a start\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Trying bigger models ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8169e6e835d348eb8f1d3580baca9ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b45840065ee45a78db0a6adbae466d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f217760233415cae390d19dc1a9480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8722d8cd88c45df9a455d792f464e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6e0b5028e74438a80ef381e8fb93e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load Longformer tokenizer and model (base version, 4096 token support)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2)  # 2 labels for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the entire script (max_length increased to 4096)\n",
    "tokenized = tokenizer(\n",
    "    data['script'].tolist(),\n",
    "    max_length=4096,  # Longformer supports up to 4096 tokens\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "data['input_ids'] = tokenized['input_ids']\n",
    "data['attention_mask'] = tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.input_ids = torch.stack(data['input_ids'].tolist())\n",
    "        self.attention_mask = torch.stack(data['attention_mask'].tolist())\n",
    "        self.labels = torch.tensor(data['passed_bechdel'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = ScriptDataset(train_data)\n",
    "val_dataset = ScriptDataset(val_data)\n",
    "test_dataset = ScriptDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [03:59<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 0.6232423501068287\n",
      "Epoch 1: Validation Loss = 0.6969125866889954, Validation Accuracy = 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [03:59<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss = 0.6146960559855686\n",
      "Epoch 2: Validation Loss = 0.6896079381306967, Validation Accuracy = 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [03:59<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss = 0.5928333649474583\n",
      "Epoch 3: Validation Loss = 0.6704256335894266, Validation Accuracy = 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [03:58<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss = 0.5772817124811451\n",
      "Epoch 4: Validation Loss = 0.7360468953847885, Validation Accuracy = 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [03:59<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss = 0.5363863555902846\n",
      "Epoch 5: Validation Loss = 0.8584823037187258, Validation Accuracy = 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [03:59<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss = 0.4790857340512651\n",
      "Epoch 6: Validation Loss = 0.8743902146816254, Validation Accuracy = 0.62\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5) # changed 1e -> 3e\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(6):  # 6 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Training Loss = {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1}: Validation Loss = {val_loss / len(val_loader)}, Validation Accuracy = {val_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- stella_en_v5 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856a9d4035344955b25713ef7b33c83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f690c9d55fc9409babc2d05836a0044f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc513eacc954fbfbc13872610974c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124237fa845a470c9faa9ed63968ceb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb5ffbf1fa641a28b87a39d83ae4e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369d7329f32346329eaf14beb785599b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/370 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f422680736b449ce968eb7b25656ae86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758b304995a34650971f0db5cd88eb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at blevlabs/stella_en_v5 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForSequenceClassification(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151646, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=1536, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = \"blevlabs/stella_en_v5\"  # or the specific model you want to use\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
