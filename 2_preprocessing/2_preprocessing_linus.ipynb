{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by: \n",
    "\n",
    "Date: 2024-12-07 \n",
    "\n",
    "Latest change when and what:\n",
    "\n",
    "Notes:\n",
    "\n",
    "# 2. Preprocessing\n",
    "\n",
    "Cleaning the data, tokenizing it, splitting it into test, train and validation, and finally embedding the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import kagglehub\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>decade</th>\n",
       "      <th>5_year_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1307</td>\n",
       "      <td>Nosferatu, eine Symphonie des Grauens</td>\n",
       "      <td>13442</td>\n",
       "      <td>1922</td>\n",
       "      <td>0</td>\n",
       "      <td>Nosferatu_0013442.txt</td>\n",
       "      <td>\\n\\n                              1922\\n\\n\\n\\n...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1305</td>\n",
       "      <td>Phantom of the Opera, The</td>\n",
       "      <td>16220</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Phantom of the Opera_0016220.txt</td>\n",
       "      <td>The Phantom of the Opera\\n\\nTHE PHANTOM OF THE...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1308</td>\n",
       "      <td>Battleship Potemkin</td>\n",
       "      <td>15648</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>Battleship Potemkin_0015648.txt</td>\n",
       "      <td>Battleship Potemkin\\n\\nScenario and script by ...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5514</td>\n",
       "      <td>Lost World, The</td>\n",
       "      <td>16039</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Lost World_0016039.txt</td>\n",
       "      <td>THE LOST WORLD\\nJURASSIC PARK\\n\\nscreenplay by...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1267</td>\n",
       "      <td>Metropolis</td>\n",
       "      <td>17136</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>Metropolis_0017136.txt</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  ...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating    id                                  title  imdbid  year  \\\n",
       "0       2  1307  Nosferatu, eine Symphonie des Grauens   13442  1922   \n",
       "1       2  1305              Phantom of the Opera, The   16220  1925   \n",
       "2       0  1308                    Battleship Potemkin   15648  1925   \n",
       "3       2  5514                        Lost World, The   16039  1925   \n",
       "4       1  1267                             Metropolis   17136  1927   \n",
       "\n",
       "   passed_bechdel                       script_filename  \\\n",
       "0               0                 Nosferatu_0013442.txt   \n",
       "1               0  The Phantom of the Opera_0016220.txt   \n",
       "2               0       Battleship Potemkin_0015648.txt   \n",
       "3               0            The Lost World_0016039.txt   \n",
       "4               0                Metropolis_0017136.txt   \n",
       "\n",
       "                                              script  decade  5_year_bin  \n",
       "0  \\n\\n                              1922\\n\\n\\n\\n...    1920        1920  \n",
       "1  The Phantom of the Opera\\n\\nTHE PHANTOM OF THE...    1920        1925  \n",
       "2  Battleship Potemkin\\n\\nScenario and script by ...    1920        1925  \n",
       "3  THE LOST WORLD\\nJURASSIC PARK\\n\\nscreenplay by...    1920        1925  \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  ...    1920        1925  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"../1_data_acquisition/data/labels_and_scripts.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning the data\n",
    "\n",
    "Removing '/n', lowercasing, removing special characters, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>decade</th>\n",
       "      <th>5_year_bin</th>\n",
       "      <th>script_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1307</td>\n",
       "      <td>Nosferatu, eine Symphonie des Grauens</td>\n",
       "      <td>13442</td>\n",
       "      <td>1922</td>\n",
       "      <td>0</td>\n",
       "      <td>Nosferatu_0013442.txt</td>\n",
       "      <td>1922 nosferatu cast count dracula the vampirem...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1920</td>\n",
       "      <td>[101, 4798, 16839, 27709, 8525, 3459, 4175, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1305</td>\n",
       "      <td>Phantom of the Opera, The</td>\n",
       "      <td>16220</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Phantom of the Opera_0016220.txt</td>\n",
       "      <td>the phantom of the opera the phantom of the op...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 1996, 11588, 1997, 1996, 3850, 1996, 115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1308</td>\n",
       "      <td>Battleship Potemkin</td>\n",
       "      <td>15648</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>Battleship Potemkin_0015648.txt</td>\n",
       "      <td>battleship potemkin scenario and script by ser...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 17224, 8962, 6633, 4939, 11967, 1998, 58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5514</td>\n",
       "      <td>Lost World, The</td>\n",
       "      <td>16039</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Lost World_0016039.txt</td>\n",
       "      <td>the lost world jurassic park screenplay by dav...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 1996, 2439, 2088, 19996, 2380, 9000, 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1267</td>\n",
       "      <td>Metropolis</td>\n",
       "      <td>17136</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>Metropolis_0017136.txt</td>\n",
       "      <td>metropolis by corey mandell fade in ext manhat...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 18236, 2011, 18132, 2158, 12662, 12985, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating    id                                  title  imdbid  year  \\\n",
       "0       2  1307  Nosferatu, eine Symphonie des Grauens   13442  1922   \n",
       "1       2  1305              Phantom of the Opera, The   16220  1925   \n",
       "2       0  1308                    Battleship Potemkin   15648  1925   \n",
       "3       2  5514                        Lost World, The   16039  1925   \n",
       "4       1  1267                             Metropolis   17136  1927   \n",
       "\n",
       "   passed_bechdel                       script_filename  \\\n",
       "0               0                 Nosferatu_0013442.txt   \n",
       "1               0  The Phantom of the Opera_0016220.txt   \n",
       "2               0       Battleship Potemkin_0015648.txt   \n",
       "3               0            The Lost World_0016039.txt   \n",
       "4               0                Metropolis_0017136.txt   \n",
       "\n",
       "                                              script  decade  5_year_bin  \\\n",
       "0  1922 nosferatu cast count dracula the vampirem...    1920        1920   \n",
       "1  the phantom of the opera the phantom of the op...    1920        1925   \n",
       "2  battleship potemkin scenario and script by ser...    1920        1925   \n",
       "3  the lost world jurassic park screenplay by dav...    1920        1925   \n",
       "4  metropolis by corey mandell fade in ext manhat...    1920        1925   \n",
       "\n",
       "                                       script_tokens  \n",
       "0  [101, 4798, 16839, 27709, 8525, 3459, 4175, 18...  \n",
       "1  [101, 1996, 11588, 1997, 1996, 3850, 1996, 115...  \n",
       "2  [101, 17224, 8962, 6633, 4939, 11967, 1998, 58...  \n",
       "3  [101, 1996, 2439, 2088, 19996, 2380, 9000, 201...  \n",
       "4  [101, 18236, 2011, 18132, 2158, 12662, 12985, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"script\"] = (\n",
    "    data[\"script\"]\n",
    "    .str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n",
    "    .str.replace('\\n', ' ')                   # Remove newlines\n",
    "    .str.lower()                             # Convert to lowercase\n",
    "    .str.replace(r'\\s+', ' ', regex=True)    # Replace multiple spaces with a single space\n",
    "    .str.strip()                             # Remove leading/trailing spaces\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing (500 tokens)\n",
    "\n",
    "Tokenizing using a pre-trained BERT tokenizer from transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_length=500: Specifies the maximum number of tokens to include.\n",
    "\n",
    "truncation=True: Ensures that if the text exceeds 500 tokens, it will be truncated to fit the specified length.\n",
    "\n",
    "add_special_tokens=True: Includes any special tokens required by the model, such as [CLS] and [SEP] for BERT.\n",
    "\n",
    "Transformed-based models like BERT need inputs of same length -> pad & attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>imdbid</th>\n",
       "      <th>year</th>\n",
       "      <th>passed_bechdel</th>\n",
       "      <th>script_filename</th>\n",
       "      <th>script</th>\n",
       "      <th>decade</th>\n",
       "      <th>5_year_bin</th>\n",
       "      <th>script_tokens</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1307</td>\n",
       "      <td>Nosferatu, eine Symphonie des Grauens</td>\n",
       "      <td>13442</td>\n",
       "      <td>1922</td>\n",
       "      <td>0</td>\n",
       "      <td>Nosferatu_0013442.txt</td>\n",
       "      <td>1922 nosferatu cast count dracula the vampirem...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1920</td>\n",
       "      <td>[101, 4798, 16839, 27709, 8525, 3459, 4175, 18...</td>\n",
       "      <td>[tensor(101), tensor(4798), tensor(16839), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1305</td>\n",
       "      <td>Phantom of the Opera, The</td>\n",
       "      <td>16220</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Phantom of the Opera_0016220.txt</td>\n",
       "      <td>the phantom of the opera the phantom of the op...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 1996, 11588, 1997, 1996, 3850, 1996, 115...</td>\n",
       "      <td>[tensor(101), tensor(1996), tensor(11588), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1308</td>\n",
       "      <td>Battleship Potemkin</td>\n",
       "      <td>15648</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>Battleship Potemkin_0015648.txt</td>\n",
       "      <td>battleship potemkin scenario and script by ser...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 17224, 8962, 6633, 4939, 11967, 1998, 58...</td>\n",
       "      <td>[tensor(101), tensor(17224), tensor(8962), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5514</td>\n",
       "      <td>Lost World, The</td>\n",
       "      <td>16039</td>\n",
       "      <td>1925</td>\n",
       "      <td>0</td>\n",
       "      <td>The Lost World_0016039.txt</td>\n",
       "      <td>the lost world jurassic park screenplay by dav...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 1996, 2439, 2088, 19996, 2380, 9000, 201...</td>\n",
       "      <td>[tensor(101), tensor(1996), tensor(2439), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1267</td>\n",
       "      <td>Metropolis</td>\n",
       "      <td>17136</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>Metropolis_0017136.txt</td>\n",
       "      <td>metropolis by corey mandell fade in ext manhat...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1925</td>\n",
       "      <td>[101, 18236, 2011, 18132, 2158, 12662, 12985, ...</td>\n",
       "      <td>[tensor(101), tensor(18236), tensor(2011), ten...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating    id                                  title  imdbid  year  \\\n",
       "0       2  1307  Nosferatu, eine Symphonie des Grauens   13442  1922   \n",
       "1       2  1305              Phantom of the Opera, The   16220  1925   \n",
       "2       0  1308                    Battleship Potemkin   15648  1925   \n",
       "3       2  5514                        Lost World, The   16039  1925   \n",
       "4       1  1267                             Metropolis   17136  1927   \n",
       "\n",
       "   passed_bechdel                       script_filename  \\\n",
       "0               0                 Nosferatu_0013442.txt   \n",
       "1               0  The Phantom of the Opera_0016220.txt   \n",
       "2               0       Battleship Potemkin_0015648.txt   \n",
       "3               0            The Lost World_0016039.txt   \n",
       "4               0                Metropolis_0017136.txt   \n",
       "\n",
       "                                              script  decade  5_year_bin  \\\n",
       "0  1922 nosferatu cast count dracula the vampirem...    1920        1920   \n",
       "1  the phantom of the opera the phantom of the op...    1920        1925   \n",
       "2  battleship potemkin scenario and script by ser...    1920        1925   \n",
       "3  the lost world jurassic park screenplay by dav...    1920        1925   \n",
       "4  metropolis by corey mandell fade in ext manhat...    1920        1925   \n",
       "\n",
       "                                       script_tokens  \\\n",
       "0  [101, 4798, 16839, 27709, 8525, 3459, 4175, 18...   \n",
       "1  [101, 1996, 11588, 1997, 1996, 3850, 1996, 115...   \n",
       "2  [101, 17224, 8962, 6633, 4939, 11967, 1998, 58...   \n",
       "3  [101, 1996, 2439, 2088, 19996, 2380, 9000, 201...   \n",
       "4  [101, 18236, 2011, 18132, 2158, 12662, 12985, ...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [tensor(101), tensor(4798), tensor(16839), ten...   \n",
       "1  [tensor(101), tensor(1996), tensor(11588), ten...   \n",
       "2  [tensor(101), tensor(17224), tensor(8962), ten...   \n",
       "3  [tensor(101), tensor(1996), tensor(2439), tens...   \n",
       "4  [tensor(101), tensor(18236), tensor(2011), ten...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_encode(text, tokenizer, max_length=500):\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "    return encoded['input_ids'][0], encoded['attention_mask'][0]\n",
    "\n",
    "# Apply to all scripts\n",
    "data[['input_ids', 'attention_mask']] = data['script'].apply(\n",
    "    lambda x: pd.Series(tokenize_and_encode(x, tokenizer, max_length=500))\n",
    ")\n",
    "\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking\n",
    "len(data['script_tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Splitting the data into test, train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1424\n",
      "Validation size: 178\n",
      "Test size: 179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Split the data into train and temp (validation+test) sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)  # 20% for validation+test\n",
    "\n",
    "# Step 2: Split temp_data into validation and test sets (10% each)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)  # 50% of temp (10% of original)\n",
    "\n",
    "# Display the sizes of each set\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the test, train and val datasets\n",
    "train_data.to_csv(\"train.csv\", index= False)\n",
    "test_data.to_csv(\"test.csv\", index= False)\n",
    "val_data.to_csv(\"validation.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1718a9e6e584e37a35d9c88e681d43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.input_ids = torch.stack(data['input_ids'].tolist())\n",
    "        self.attention_mask = torch.stack(data['attention_mask'].tolist())\n",
    "        self.labels = torch.tensor(data['passed_bechdel'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = ScriptDataset(train_data)\n",
    "val_dataset = ScriptDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.5384442210197449\n",
      "Epoch 2: Loss = 0.26806384325027466\n",
      "Epoch 3: Loss = 0.07606004923582077\n",
      "Epoch 4: Loss = 0.1101921796798706\n",
      "Epoch 5: Loss = 0.04116030037403107\n",
      "Epoch 6: Loss = 0.02917688526213169\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(6):  # 3 epochs as a start\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------- New stuff above, old stuff below -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trying to feed the tokens of one script to distilbert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_script = train_data.iloc[0]\n",
    "one_script['script_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Prepare data collator for padding sequences\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Define Trainer object for training the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "trainer.save_model('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Embedding the data\n",
    "\n",
    "Because we are using a pre-trained model and tokenizer, we need to first get the token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained tokenizer (e.g., BERT tokenizer)\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to convert words to token IDs using the tokenizer\n",
    "def words_to_token_ids(words):\n",
    "    # Tokenize the words into token IDs\n",
    "    return tokenizer.convert_tokens_to_ids(words)\n",
    "\n",
    "# Apply the function to your tokenized column (assuming `script_tokens` contains words)\n",
    "train_data[\"script_token_ids\"] = train_data[\"script_tokens\"].apply(words_to_token_ids)\n",
    "val_data[\"script_token_ids\"] = val_data[\"script_tokens\"].apply(words_to_token_ids)\n",
    "test_data[\"script_token_ids\"] = test_data[\"script_tokens\"].apply(words_to_token_ids)\n",
    "\n",
    "# Check the result\n",
    "print(train_data[\"script_token_ids\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding the script token ids\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "#model_name = 'bert-base-uncased'\n",
    "#model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to obtain embeddings for token IDs\n",
    "def get_embeddings(token_ids):\n",
    "    # Convert list of token IDs to a tensor\n",
    "    tokens_tensor = torch.tensor([token_ids])  # Add a batch dimension\n",
    "    \n",
    "    # Create an attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = (tokens_tensor != tokenizer.pad_token_id).int()\n",
    "    \n",
    "    # Pass through the model (no gradient computation to save memory)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=tokens_tensor, attention_mask=attention_mask)\n",
    "    \n",
    "    # Extract the embeddings (usually from the last hidden layer)\n",
    "    # The shape of outputs.last_hidden_state is (batch_size, sequence_length, hidden_size)\n",
    "    # We usually take the embeddings of the [CLS] token (index 0)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]  # [CLS] token's embedding\n",
    "\n",
    "    return cls_embedding\n",
    "\n",
    "# Apply the function to the tokenized column\n",
    "train_data[\"embeddings\"] = train_data[\"script_token_ids\"].apply(get_embeddings)\n",
    "val_data[\"embeddings\"] = val_data[\"script_token_ids\"].apply(get_embeddings)\n",
    "test_data[\"embeddings\"] = test_data[\"script_token_ids\"].apply(get_embeddings)\n",
    "\n",
    "# Check the result\n",
    "print(train_data[\"embeddings\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "train_data.to_csv(\"train_embedded.csv\", index= False)\n",
    "test_data.to_csv(\"test_embedded.csv\", index= False)\n",
    "val_data.to_csv(\"validation_embedded.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_embedded.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['embeddings'] # format is slightly different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing a small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "# Load the pre-trained DistilBERT model with a classification head\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DON't know about the code below......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
