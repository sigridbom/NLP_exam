{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_5705/3782884818.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Mixed precision scaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5705/3782884818.py:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n",
      "Initializing global attention on CLS token...\n",
      "/tmp/ipykernel_5705/3782884818.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision during evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7058, Train Accuracy: 0.4979\n",
      "Validation Loss: 0.6932, Validation Accuracy: 0.5028, Validation F1: 0.6692\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5705/3782884818.py:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n",
      "/tmp/ipykernel_5705/3782884818.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision during evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6989, Train Accuracy: 0.4993\n",
      "Validation Loss: 0.7021, Validation Accuracy: 0.5028, Validation F1: 0.6692\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5705/3782884818.py:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n",
      "/tmp/ipykernel_5705/3782884818.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision during evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6980, Train Accuracy: 0.5021\n",
      "Validation Loss: 0.6941, Validation Accuracy: 0.4972, Validation F1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5705/3782884818.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision during evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6943, Test Accuracy: 0.4944, Test F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Environment variable to improve memory allocation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Tokenizer and Model Initialization\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2)\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "model.config.attention_window = [256] * len(model.config.attention_window)  # Reduce attention window to address outofmemory issues\n",
    "model = model.to(\"cuda\")  # Make sure model runs on GPU\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, scripts, labels, tokenizer, max_length=2048): #reduced to 2048 to address outofmemory issues\n",
    "        self.scripts = scripts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scripts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.scripts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# DataLoader Preparation Function\n",
    "def prepare_dataloader(data, tokenizer, max_length, batch_size):\n",
    "    dataset = ScriptDataset(\n",
    "        scripts=data[\"script\"].tolist(),\n",
    "        labels=data[\"passed_bechdel\"].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load Datasets\n",
    "train_data = pd.read_csv(\"../2_preprocessing/train_case_sensitive.csv\")\n",
    "val_data = pd.read_csv(\"../2_preprocessing/validation_case_sensitive.csv\")\n",
    "test_data = pd.read_csv(\"../2_preprocessing/test_case_sensitive.csv\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = prepare_dataloader(train_data, tokenizer, max_length=2048, batch_size=1)\n",
    "val_loader = prepare_dataloader(val_data, tokenizer, max_length=2048, batch_size=1)\n",
    "test_loader = prepare_dataloader(test_data, tokenizer, max_length=2048, batch_size=1)\n",
    "\n",
    "# Optimizer, Loss, and Scaler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()  # Mixed precision scaler\n",
    "gradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n",
    "\n",
    "# Training Loop\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"label\"].to(\"cuda\")\n",
    "\n",
    "        with autocast():  # Enable mixed precision\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss = loss / gradient_accumulation_steps  # Scale loss for accumulation\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        total_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        # Periodically clear CUDA cache to manage memory\n",
    "        if step % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n",
    "\n",
    "# Evaluation Loop\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "            labels = batch[\"label\"].to(\"cuda\")\n",
    "\n",
    "            with autocast():  # Mixed precision during evaluation\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
    "            all_preds.extend(outputs.logits.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return total_loss / len(dataloader), accuracy, f1\n",
    "\n",
    "# Main Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_loss, train_accuracy = train(model, train_loader)\n",
    "    val_loss, val_accuracy, val_f1 = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "# Evaluate on Test Data\n",
    "test_loss, test_accuracy, test_f1 = evaluate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the model with 3072 tokens and a text prompt explaining the Bechdel test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7027, Train Accuracy: 0.4718\n",
      "Validation Loss: 0.6949, Validation Accuracy: 0.4972, Validation F1: 0.0000\n",
      "Epoch 2/3\n",
      "Train Loss: 0.7007, Train Accuracy: 0.4859\n",
      "Validation Loss: 0.6970, Validation Accuracy: 0.5028, Validation F1: 0.6692\n",
      "Epoch 3/3\n",
      "Train Loss: 0.7004, Train Accuracy: 0.4831\n",
      "Validation Loss: 0.6935, Validation Accuracy: 0.4972, Validation F1: 0.0000\n",
      "Test Loss: 0.6936, Test Accuracy: 0.4944, Test F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Environment variable for memory allocation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Tokenizer and Model Initialization\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.attention_window = [256] * len(model.config.attention_window)  # Reduce attention window for OOM prevention\n",
    "model = torch.nn.DataParallel(model).to(\"cuda\")  # Use all GPUs\n",
    "\n",
    "# Dataset with Optional Instruction Prompt\n",
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, scripts, labels, tokenizer, max_length=3072, use_prompt=False): # adjust max_length if getting OOM issues\n",
    "        self.scripts = scripts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_prompt = use_prompt\n",
    "        self.prompt = (\n",
    "            \"Does the following manuscript pass the Bechdel test or not? \"\n",
    "            \"A movie passes the Bechdel test if it fulfills three requirements: \"\n",
    "            \"1) There are at least two named female characters, 2) They have a conversation, \"\n",
    "            \"3) About something other than a man. Manuscript: \"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scripts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.prompt + self.scripts[idx] if self.use_prompt else self.scripts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# DataLoader Preparation\n",
    "def prepare_dataloader(data, tokenizer, max_length, batch_size, use_prompt=False):\n",
    "    dataset = ScriptDataset(\n",
    "        scripts=data[\"script\"].tolist(),\n",
    "        labels=data[\"passed_bechdel\"].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        use_prompt=use_prompt\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(\"../2_preprocessing/train_case_sensitive.csv\")\n",
    "val_data = pd.read_csv(\"../2_preprocessing/validation_case_sensitive.csv\")\n",
    "test_data = pd.read_csv(\"../2_preprocessing/test_case_sensitive.csv\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = prepare_dataloader(train_data, tokenizer, max_length=3072, batch_size=1, use_prompt=True)\n",
    "val_loader = prepare_dataloader(val_data, tokenizer, max_length=3072, batch_size=1, use_prompt=True)\n",
    "test_loader = prepare_dataloader(test_data, tokenizer, max_length=3072, batch_size=1, use_prompt=True)\n",
    "\n",
    "# Optimizer, Loss, and Scaler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()  # Mixed precision scaler\n",
    "gradient_accumulation_steps = 4  # Accumulate gradients over 4 steps\n",
    "\n",
    "# Training Loop\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"label\"].to(\"cuda\")\n",
    "\n",
    "        with autocast(\"cuda\"):  # Enable mixed precision\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss = loss / gradient_accumulation_steps  # Scale loss for accumulation\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        total_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        # Periodically clear CUDA cache\n",
    "        if step % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n",
    "\n",
    "# Evaluation Loop\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "            labels = batch[\"label\"].to(\"cuda\")\n",
    "\n",
    "            with autocast(\"cuda\"):  # Mixed precision\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
    "            all_preds.extend(outputs.logits.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return total_loss / len(dataloader), accuracy, f1\n",
    "\n",
    "# Main Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_loss, train_accuracy = train(model, train_loader)\n",
    "    val_loss, val_accuracy, val_f1 = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "# Evaluate on Test Data\n",
    "test_loss, test_accuracy, test_f1 = evaluate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1: {test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
