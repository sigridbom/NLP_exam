{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model with 3072 tokens, no text prompt \n",
    "### run time â‰ˆ 80 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518b1b117aa346518c9fd915edfa004f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c7fa2f76c34fe6a62ecd4f5e6f241b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b7e47002b84989b977ff6f9f9770fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c96826a9de46768f025fa4586d8d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ce9c9496174ac99e65ab844fb7ba0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_window \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m256\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mattention_window)\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongformer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Dataset with Optional Instruction Prompt\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Environment variable for memory allocation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Tokenizer and Model Initialization\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.attention_window = [256] * len(model.config.attention_window)\n",
    "model = torch.nn.DataParallel(model).to(\"cuda\")\n",
    "model_name = 'longformer'\n",
    "\n",
    "# Dataset with Optional Instruction Prompt\n",
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, scripts, labels, tokenizer, max_length=3, use_prompt=False): # use prompt is set to False here\n",
    "        self.scripts = scripts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_prompt = use_prompt\n",
    "        self.prompt = (\n",
    "            \"Does the following manuscript pass the Bechdel test or not? \"\n",
    "            \"A movie passes the Bechdel test if it fulfills three requirements: \"\n",
    "            \"1) There are at least two named female characters, 2) They have a conversation, \"\n",
    "            \"3) About something other than a man. Manuscript: \"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scripts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.prompt + self.scripts[idx] if self.use_prompt else self.scripts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# DataLoader Preparation\n",
    "def prepare_dataloader(data, tokenizer, max_length, batch_size, use_prompt=False):\n",
    "    dataset = ScriptDataset(\n",
    "        scripts=data[\"script\"].tolist(),\n",
    "        labels=data[\"passed_bechdel\"].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        use_prompt=use_prompt\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(\"../2_preprocessing/train_case_sensitive.csv\")\n",
    "val_data = pd.read_csv(\"../2_preprocessing/validation_case_sensitive.csv\")\n",
    "test_data = pd.read_csv(\"../2_preprocessing/test_case_sensitive.csv\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = prepare_dataloader(train_data, tokenizer, max_length=3072, batch_size=1, use_prompt=True)\n",
    "val_loader = prepare_dataloader(val_data, tokenizer, max_length=3072, batch_size=1, use_prompt=True)\n",
    "test_loader = prepare_dataloader(test_data, tokenizer, max_length=3072, batch_size=1, use_prompt=True)\n",
    "\n",
    "# Optimizer, Loss, and Scaler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.001) # decreased learning rate and weight decay to address regularization issues\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(conf_matrix, title, dataset_name):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Fail\", \"Pass\"], yticklabels=[\"Fail\", \"Pass\"])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(f\"{title} - {dataset_name}\")\n",
    "    plt.show()\n",
    "\n",
    "# Training Loop\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"label\"].to(\"cuda\")\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        total_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n",
    "\n",
    "# Evaluation Loop\n",
    "def evaluate(model, dataloader, dataset_name):\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "    \"model\", \"split\", \"accuracy\", \"f1_score\", \"roc_auc\", \n",
    "    \"true_negatives\", \"true_positives\", \"false_positives\", \"false_negatives\", \"precision\", \"recall\", \n",
    "    \"true_negative_rate\", \"false_negative_rate\", \"false_positive_rate\"])\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "            labels = batch[\"label\"].to(\"cuda\")\n",
    "\n",
    "            with autocast(\"cuda\"):\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.logits.argmax(1) == labels).sum().item()\n",
    "            all_preds.extend(outputs.logits.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(torch.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy())\n",
    "\n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"\\n{dataset_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "\n",
    "    #plot_confusion_matrix(conf_matrix, \"Confusion Matrix\", dataset_name)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    results_df.loc[len(results_df)] = {\n",
    "        \"model\": model_name, \n",
    "        \"split\": dataset_name, # \n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1, \n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"true_negatives\": tn, \n",
    "        \"true_positives\": tp,\n",
    "        \"false_positives\": fp,\n",
    "        \"false_negatives\": fn, \n",
    "        \"precision\": tp/(tp+fp), \n",
    "        \"recall\": tp/(tp+fn), \n",
    "        \"true_negative_rate\": tn/(tn+fp), \n",
    "        \"false_negative_rate\": fn/(fn+tp), \n",
    "        \"false_positive_rate\": fp/(fp+tn)\n",
    "    }\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"Failed\", \"Passed\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    disp.ax_.set_title(f\"{model_name}, data split: {datasplit}\")\n",
    "    plt.savefig(f'./output/figures/{model_name}_{datasplit}_confusion_matrix.jpeg')\n",
    "    plt.show()\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, f1, roc_auc\n",
    "\n",
    "# Main Training Loop\n",
    "epochs = 1 # CHANGE BACK to 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_loss, train_accuracy = train(model, train_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    val_loss, val_accuracy, val_f1, val_roc_auc = evaluate(model, val_loader, \"validation\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation F1: {val_f1:.4f}, Validation ROC-AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "# Evaluate on Test Data\n",
    "test_loss, test_accuracy, test_f1, test_roc_auc = evaluate(model, test_loader, \"test\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1: {test_f1:.4f}, Test ROC-AUC: {test_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3072 tokens and text prompt explaining Bechdel test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7df644842b44f2a4833097cc2e52b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589ad9f66cae4853a9985ad352b2eecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef5608588bb45c28b3d372b07515cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9a25dda69b4e75a45c7a712a098db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96dc2c13d07441fa640254769f4f5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6977\n",
      "Epoch 2/3\n",
      "Train Loss: 0.6980\n",
      "Epoch 3/3\n",
      "Train Loss: 0.6940\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Save logits and labels for second chunk\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m val_logits, val_labels \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m test_logits, test_labels \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Save results for use in the second chunk\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 127\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m    124\u001b[0m         all_logits\u001b[38;5;241m.\u001b[39mextend(outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    125\u001b[0m         all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(all_logits), np\u001b[38;5;241m.\u001b[39marray(all_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Environment variable for memory allocation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Tokenizer and Model Initialization\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.attention_window = [256] * len(model.config.attention_window)\n",
    "model = torch.nn.DataParallel(model).to(\"cuda\")\n",
    "\n",
    "# Dataset with Optional Instruction Prompt\n",
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, scripts, labels, tokenizer, max_length=3072, use_prompt=True): # use prompt is set to True here\n",
    "        self.scripts = scripts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_prompt = use_prompt\n",
    "        self.prompt = (\n",
    "            \"Does the following manuscript pass the Bechdel test or not? \"\n",
    "            \"A movie passes the Bechdel test if it fulfills three requirements: \"\n",
    "            \"1) There are at least two named female characters, 2) They have a conversation, \"\n",
    "            \"3) About something other than a man. Manuscript: \"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scripts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.prompt + self.scripts[idx] if self.use_prompt else self.scripts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# DataLoader Preparation\n",
    "def prepare_dataloader(data, tokenizer, max_length, batch_size, use_prompt=False):\n",
    "    dataset = ScriptDataset(\n",
    "        scripts=data[\"script\"].tolist(),\n",
    "        labels=data[\"passed_bechdel\"].tolist(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        use_prompt=use_prompt\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(\"../2_preprocessing/train_case_sensitive.csv\")\n",
    "val_data = pd.read_csv(\"../2_preprocessing/validation_case_sensitive.csv\")\n",
    "test_data = pd.read_csv(\"../2_preprocessing/test_case_sensitive.csv\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = prepare_dataloader(train_data, tokenizer, max_length=3072, batch_size=1, use_prompt=True)\n",
    "val_loader = prepare_dataloader(val_data, tokenizer, max_length=3072, batch_size=1, use_prompt=True)\n",
    "test_loader = prepare_dataloader(test_data, tokenizer, max_length=3072, batch_size=1, use_prompt=True)\n",
    "\n",
    "# Optimizer, Loss, and Scaler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.001)  # decreased learning rate and weight decay to address regularization issues\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Training Loop\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "        labels = batch[\"label\"].to(\"cuda\")\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Evaluation Loop (outputs logits and labels only for second chunk)\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "            labels = batch[\"label\"].to(\"cuda\")\n",
    "\n",
    "            with autocast(\"cuda\"):\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            all_logits.extend(outputs.logits.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return np.array(all_logits), np.array(all_labels)\n",
    "\n",
    "# Main Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_loss = train(model, train_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Save logits and labels\n",
    "val_logits, val_labels = evaluate(model, val_loader)\n",
    "test_logits, test_labels = evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits and labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Re-run the evaluation for validation and test data\n",
    "val_logits, val_labels = evaluate(model, val_loader)\n",
    "test_logits, test_labels = evaluate(model, test_loader)\n",
    "\n",
    "# Save the logits and labels to .npy files\n",
    "np.save(\"./output/val_logits.npy\", val_logits)\n",
    "np.save(\"./output/val_labels.npy\", val_labels)\n",
    "np.save(\"./output/test_logits.npy\", test_logits)\n",
    "np.save(\"./output/test_labels.npy\", test_labels)\n",
    "\n",
    "print(\"Logits and labels saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'longformer'\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2)  \n",
    "results_df = pd.DataFrame(columns=[\n",
    "    \"model\", \"split\", \"accuracy\", \"f1_score\", \"roc_auc\", \n",
    "    \"true_negatives\", \"true_positives\", \"false_positives\", \"false_negatives\", \"precision\", \"recall\", \n",
    "    \"true_negative_rate\", \"false_negative_rate\", \"false_positive_rate\"])\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    # ROC-AUC Score (binary classification assumed)\n",
    "    roc_auc = roc_auc_score(labels, logits[:, 1])#, multi_class='ovr')\n",
    "    # Plot and save confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Failed\", \"Passed\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    disp.ax_.set_title(f\"{model_name}, data split: {datasplit}\")\n",
    "    plt.savefig(f'./output/figures/{model_name}_{datasplit}_confusion_matrix.jpeg')\n",
    "    #plt.savefig(f\"./output/figures/distilroberta_confusion_matrix_epoch_{trainer.state.epoch}.png\")\n",
    "    #plt.close()\n",
    "    plt.show()\n",
    "    # Append results to DataFrame\n",
    "    results_df.loc[len(results_df)] = {\n",
    "        \"model\": model_name, \n",
    "        \"split\": datasplit, # \n",
    "        \"accuracy\": acc,\n",
    "        \"f1_score\": f1, \n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"true_negatives\": tn, \n",
    "        \"true_positives\": tp,\n",
    "        \"false_positives\": fp,\n",
    "        \"false_negatives\": fn, \n",
    "        \"precision\": precision, \n",
    "        \"recall\": recall, \n",
    "        \"true_negative_rate\": tn/(tn+fp), \n",
    "        \"false_negative_rate\": fn/(fn+tp), \n",
    "        \"false_positive_rate\": fp/(fp+tn)\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
